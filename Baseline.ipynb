{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71CjPOg5prFc"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "\n",
        "#==============================================================================\n",
        "# DATA LOADING\n",
        "#==============================================================================\n",
        "\n",
        "def load_arc_datasets():\n",
        "    \"\"\"Load both ARC Challenge and Easy datasets.\"\"\"\n",
        "    print(\"Loading ARC datasets...\")\n",
        "\n",
        "    # ARC Challenge\n",
        "    arc_data = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
        "    arc_df = arc_data.to_pandas()\n",
        "    arc_df = arc_df.drop_duplicates(subset=['question'])\n",
        "    arc_df[\"choices_dic\"] = arc_df[\"choices\"]\n",
        "    arc_df[\"choices\"] = arc_df[\"choices\"].apply(lambda x: x[\"text\"])\n",
        "    arc_df[\"subject\"] = \"science\"\n",
        "    print(f\"ARC Challenge shape: {arc_df.shape}\")\n",
        "\n",
        "    # ARC Easy\n",
        "    arc_data_easy = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\", split=\"test\")\n",
        "    arc_df_easy = arc_data_easy.to_pandas()\n",
        "    arc_df_easy = arc_df_easy.drop_duplicates(subset=['question'])\n",
        "    arc_df_easy[\"choices_dic\"] = arc_df_easy[\"choices\"]\n",
        "    arc_df_easy[\"choices\"] = arc_df_easy[\"choices\"].apply(lambda x: x[\"text\"])\n",
        "    arc_df_easy[\"subject\"] = \"science\"\n",
        "    print(f\"ARC Easy shape: {arc_df_easy.shape}\")\n",
        "\n",
        "    return arc_df, arc_df_easy\n",
        "\n",
        "#==============================================================================\n",
        "# GENERATOR FUNCTIONS (Phi-2)\n",
        "#==============================================================================\n",
        "\n",
        "def format_subject(subject):\n",
        "    \"\"\"Format subject string.\"\"\"\n",
        "    l = subject.split(\"_\")\n",
        "    s = \"\"\n",
        "    for entry in l:\n",
        "        s += \" \" + entry\n",
        "    return s\n",
        "\n",
        "def build_generator_prompt(subject, target_question, target_choices, get_correct):\n",
        "    \"\"\"Build prompt for generator.\"\"\"\n",
        "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(\n",
        "        format_subject(subject))\n",
        "\n",
        "    prompt += f\"{target_question}\"\n",
        "    for i, c in enumerate(target_choices):\n",
        "        prompt += \"\\n{}\".format(c)\n",
        "\n",
        "    if get_correct:\n",
        "        prompt += \"\\nAnswer:\"\n",
        "    else:\n",
        "        prompt += \"\\nIncorrect Answer:\"\n",
        "    return prompt\n",
        "\n",
        "def get_generator_answer_probs(model, tokenizer, prompt_text, choices_list):\n",
        "    \"\"\"Get generator answer probabilities.\"\"\"\n",
        "    try:\n",
        "        input_ids = tokenizer(prompt_text, return_tensors=\"pt\", truncation=True, max_length=512).input_ids\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            input_ids = input_ids.to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids=input_ids).logits[0, -1]\n",
        "\n",
        "        choices = [f\"{chr(65+i)}\" for i, choice in enumerate(choices_list)]\n",
        "        choice_logits = []\n",
        "\n",
        "        for letter in choices:\n",
        "            token_id = tokenizer(letter, return_tensors=\"pt\").input_ids[0, -1].item()\n",
        "            choice_logits.append(logits[token_id].item())\n",
        "\n",
        "        choice_logits = torch.tensor(choice_logits).float()\n",
        "        probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
        "\n",
        "        choice_probs = {choice: prob for choice, prob in zip(choices, probs)}\n",
        "        return choice_probs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generator: {e}\")\n",
        "        # Return uniform distribution as fallback\n",
        "        choices = [f\"{chr(65+i)}\" for i, choice in enumerate(choices_list)]\n",
        "        return {choice: 1.0/len(choices) for choice in choices}\n",
        "\n",
        "def generator_probs(subject, question, choices_list, get_correct, model, tokenizer):\n",
        "    \"\"\"Get generator probabilities.\"\"\"\n",
        "    choices = [f\"{chr(65+i)}. {choice}\" for i, choice in enumerate(choices_list)]\n",
        "    prompt = build_generator_prompt(subject, question, choices, get_correct)\n",
        "    probs = get_generator_answer_probs(model, tokenizer, prompt, choices_list)\n",
        "    return probs\n",
        "\n",
        "def get_initial_generator_probs(row, model, tokenizer):\n",
        "    \"\"\"Get initial generator probabilities for correct/incorrect.\"\"\"\n",
        "    gen_init = {\"correct\": {}, \"incorrect\": {}}\n",
        "    x, y_list, subject = row[\"question\"], row[\"choices\"], row[\"subject\"]\n",
        "\n",
        "    for v in [True, False]:\n",
        "        choices_letter_prob = generator_probs(subject, x, y_list, v, model, tokenizer)\n",
        "        if v:\n",
        "            for key, val in choices_letter_prob.items():\n",
        "                gen_init[\"correct\"][key] = val\n",
        "        else:\n",
        "            for key, val in choices_letter_prob.items():\n",
        "                gen_init[\"incorrect\"][key] = val\n",
        "\n",
        "    return gen_init\n",
        "\n",
        "#==============================================================================\n",
        "# DISCRIMINATOR FUNCTIONS (Qwen2-1.5B)\n",
        "#==============================================================================\n",
        "\n",
        "def build_discriminator_prompt(subject: str, question: str, proposed_answer: str) -> str:\n",
        "    \"\"\"Build prompt for discriminator.\"\"\"\n",
        "    prompt = f\"\"\"You are an expert evaluator of questions about {format_subject(subject)}.\n",
        "Determine if the proposed answer is correct. Output ONLY 'A' or 'B'.\n",
        "Question: {question}\n",
        "Proposed Answer: {proposed_answer}\n",
        "\n",
        "Is this answer correct? Respond ONLY with:\n",
        "A. Correct\n",
        "B. Incorrect\n",
        "\n",
        "Answer:\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def get_discriminator_probs(model, tokenizer, prompt_text, choices_list):\n",
        "    \"\"\"Get discriminator probabilities.\"\"\"\n",
        "    try:\n",
        "        input_ids = tokenizer(prompt_text, return_tensors=\"pt\", truncation=True, max_length=512).input_ids\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            input_ids = input_ids.to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids=input_ids).logits[0, -1]\n",
        "\n",
        "        choice_logits = torch.tensor([\n",
        "            logits[tokenizer(\"A\").input_ids[-1]],\n",
        "            logits[tokenizer(\"B\").input_ids[-1]],\n",
        "        ]).float()\n",
        "\n",
        "        disc_dict = {\"A\":\"correct\", \"B\":\"incorrect\"}\n",
        "        probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
        "\n",
        "        choices = [\"A\", \"B\"]\n",
        "        choice_probs = {disc_dict[choice]: prob for choice, prob in zip(choices, probs)}\n",
        "\n",
        "        return choice_probs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in discriminator: {e}\")\n",
        "        return {\"correct\": 0.5, \"incorrect\": 0.5}\n",
        "\n",
        "def evaluate_answer_correctness(row, model, tokenizer):\n",
        "    \"\"\"Evaluate each answer choice with discriminator.\"\"\"\n",
        "    subject = row[\"subject\"]\n",
        "    question = row[\"question\"]\n",
        "    choices = row[\"choices\"]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for idx, answer in enumerate(choices):\n",
        "        prompt = build_discriminator_prompt(\n",
        "            subject=subject,\n",
        "            question=question,\n",
        "            proposed_answer=f\"{answer}\"\n",
        "        )\n",
        "\n",
        "        probs = get_discriminator_probs(model, tokenizer, prompt, choices)\n",
        "        answer_letter = f\"{chr(65+idx)}\"\n",
        "        results[answer_letter] = probs\n",
        "\n",
        "    return results\n",
        "\n",
        "def get_initial_discriminator_probs(row, model, tokenizer):\n",
        "    \"\"\"Get initial discriminator probabilities.\"\"\"\n",
        "    disc_init = evaluate_answer_correctness(row, model, tokenizer)\n",
        "    return disc_init\n",
        "\n",
        "#==============================================================================\n",
        "# EQUILIBRIUM SEARCH\n",
        "#==============================================================================\n",
        "\n",
        "def softmax(arr):\n",
        "    \"\"\"Numerically stable softmax.\"\"\"\n",
        "    m = np.max(arr)\n",
        "    exp_vals = np.exp(arr - m)\n",
        "    return exp_vals / np.sum(exp_vals)\n",
        "\n",
        "def equilibrium_search(gen_init, disc_init, candidates, T=20, eta_G=0.1, eta_D=0.1, lam_G=0.1, lam_D=0.01):\n",
        "    \"\"\"\n",
        "    Find Nash equilibrium between generator and discriminator\n",
        "    \"\"\"\n",
        "    gen = {\"correct\": dict(gen_init[\"correct\"]),\n",
        "           \"incorrect\": dict(gen_init[\"incorrect\"])}\n",
        "    disc = {}\n",
        "    for y in candidates:\n",
        "        disc[y] = dict(disc_init[y])\n",
        "\n",
        "    Qg = {\"correct\": {y: 0.0 for y in candidates},\n",
        "          \"incorrect\": {y: 0.0 for y in candidates}}\n",
        "    Qd = {y: {\"correct\": 0.0, \"incorrect\": 0.0} for y in candidates}\n",
        "\n",
        "    for t in range(1, T+1):\n",
        "        # Update Q\n",
        "        for v in [\"correct\", \"incorrect\"]:\n",
        "            for y in candidates:\n",
        "                Qg[v][y] += (1.0/(2.0*t)) * disc[y][v]\n",
        "\n",
        "        for y in candidates:\n",
        "            for v in [\"correct\", \"incorrect\"]:\n",
        "                Qd[y][v] += (1.0/(2.0*t)) * gen[v][y]\n",
        "\n",
        "        # Update generator policy\n",
        "        for v in [\"correct\", \"incorrect\"]:\n",
        "            logits = []\n",
        "            for y in candidates:\n",
        "                val = (Qg[v][y] + lam_G * math.log(gen_init[v][y] + 1e-12)) / (1/eta_G + lam_G)\n",
        "                logits.append(val)\n",
        "\n",
        "            new_probs = softmax(np.array(logits))\n",
        "\n",
        "            for i, y in enumerate(candidates):\n",
        "                gen[v][y] = new_probs[i]\n",
        "\n",
        "        # Update discriminator policy\n",
        "        for y in candidates:\n",
        "            logits = [\n",
        "                (Qd[y][\"correct\"] + lam_D * math.log(disc_init[y][\"correct\"] + 1e-12)) / (1/eta_D + lam_D),\n",
        "                (Qd[y][\"incorrect\"] + lam_D * math.log(disc_init[y][\"incorrect\"] + 1e-12)) / (1/eta_D + lam_D)\n",
        "            ]\n",
        "\n",
        "            probs = softmax(np.array(logits))\n",
        "            disc[y][\"correct\"] = probs[0]\n",
        "            disc[y][\"incorrect\"] = probs[1]\n",
        "\n",
        "    return gen, disc\n",
        "\n",
        "def pick_answer(gen, disc, candidates, method=\"generator\"):\n",
        "    \"\"\"Pick best answer from final policies.\"\"\"\n",
        "    if method == \"generator\":\n",
        "        best_y = None\n",
        "        best_prob = -1.0\n",
        "        for y in candidates:\n",
        "            p = gen[\"correct\"][y]\n",
        "            if p > best_prob:\n",
        "                best_prob = p\n",
        "                best_y = y\n",
        "        return best_y\n",
        "    else:\n",
        "        best_y = None\n",
        "        best_prob = -1.0\n",
        "        for y in candidates:\n",
        "            p = disc[y][\"correct\"]\n",
        "            if p > best_prob:\n",
        "                best_prob = p\n",
        "                best_y = y\n",
        "        return best_y\n",
        "\n",
        "#==============================================================================\n",
        "# MODEL LOADING\n",
        "#==============================================================================\n",
        "\n",
        "def load_model(model_name):\n",
        "    \"\"\"Load model with appropriate settings.\"\"\"\n",
        "    print(f\"Loading {model_name}...\")\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(f\"✓ {model_name} loaded successfully\")\n",
        "    return model, tokenizer\n",
        "\n",
        "#==============================================================================\n",
        "# MAIN PROCESSING\n",
        "#==============================================================================\n",
        "\n",
        "def process_dataset(generator_model, generator_tokenizer,\n",
        "                   discriminator_model, discriminator_tokenizer,\n",
        "                   df, dataset_name=\"Dataset\"):\n",
        "    \"\"\"Process dataset through consensus game.\"\"\"\n",
        "\n",
        "    total_questions = len(df)\n",
        "    results = []\n",
        "\n",
        "    # Initialize result tracking\n",
        "    gen_init_correct = 0\n",
        "    disc_init_correct = 0\n",
        "    gen_final_correct = 0\n",
        "    disc_final_correct = 0\n",
        "\n",
        "    print(f\"\\nProcessing {dataset_name}...\")\n",
        "    print(f\"Total questions: {total_questions}\")\n",
        "\n",
        "    for idx, (_, row) in enumerate(tqdm(df.iterrows(), total=len(df), desc=dataset_name)):\n",
        "        try:\n",
        "            # Get initial discriminator probabilities\n",
        "            disc_init = get_initial_discriminator_probs(row, discriminator_model, discriminator_tokenizer)\n",
        "\n",
        "            # Get initial generator probabilities\n",
        "            gen_init = get_initial_generator_probs(row, generator_model, generator_tokenizer)\n",
        "\n",
        "            # Initial answers\n",
        "            candidates = [f\"{chr(65+i)}\" for i, choice in enumerate(row[\"choices\"])]\n",
        "            gen_init_answer = pick_answer(gen_init, disc_init, candidates, method=\"generator\")\n",
        "            disc_init_answer = pick_answer(gen_init, disc_init, candidates, method=\"discriminator\")\n",
        "\n",
        "            # Equilibrium search\n",
        "            gen_final, disc_final = equilibrium_search(gen_init, disc_init, candidates, T=20)\n",
        "\n",
        "            # Final answers\n",
        "            gen_final_answer = pick_answer(gen_final, disc_final, candidates, method=\"generator\")\n",
        "            disc_final_answer = pick_answer(gen_final, disc_final, candidates, method=\"discriminator\")\n",
        "\n",
        "            # Track accuracy\n",
        "            correct_answer = row.get(\"answerKey\")\n",
        "            if correct_answer:\n",
        "                if gen_init_answer == correct_answer:\n",
        "                    gen_init_correct += 1\n",
        "                if disc_init_answer == correct_answer:\n",
        "                    disc_init_correct += 1\n",
        "                if gen_final_answer == correct_answer:\n",
        "                    gen_final_correct += 1\n",
        "                if disc_final_answer == correct_answer:\n",
        "                    disc_final_correct += 1\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                'question': row['question'],\n",
        "                'choices': row['choices'],\n",
        "                'answerKey': correct_answer,\n",
        "                'gen_init_answer': gen_init_answer,\n",
        "                'disc_init_answer': disc_init_answer,\n",
        "                'gen_answer': gen_final_answer,\n",
        "                'disc_answer': disc_final_answer,\n",
        "                'gen_init_policy': gen_init,\n",
        "                'disc_init_policy': disc_init,\n",
        "                'gen_final_policy': gen_final,\n",
        "                'disc_final_policy': disc_final\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            # Memory management\n",
        "            if (idx + 1) % 100 == 0:\n",
        "                gc.collect()\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question {idx}: {e}\")\n",
        "            # Add default values for failed questions\n",
        "            result = {\n",
        "                'question': row['question'],\n",
        "                'choices': row['choices'],\n",
        "                'answerKey': row.get('answerKey'),\n",
        "                'gen_init_answer': 'A',\n",
        "                'disc_init_answer': 'A',\n",
        "                'gen_answer': 'A',\n",
        "                'disc_answer': 'A',\n",
        "                'error': str(e)\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "    # Print accuracy summary\n",
        "    print(f\"\\n{dataset_name} Results:\")\n",
        "    print(f\"Generator (Phi-2) Performance:\")\n",
        "    print(f\"  Initial Accuracy: {gen_init_correct}/{total_questions} = {gen_init_correct/total_questions*100:.2f}%\")\n",
        "    print(f\"  Final Accuracy:   {gen_final_correct}/{total_questions} = {gen_final_correct/total_questions*100:.2f}%\")\n",
        "    print(f\"  Improvement:      {(gen_final_correct - gen_init_correct)/total_questions*100:+.2f}%\")\n",
        "\n",
        "    print(f\"\\nDiscriminator (Qwen2-1.5B) Performance:\")\n",
        "    print(f\"  Initial Accuracy: {disc_init_correct}/{total_questions} = {disc_init_correct/total_questions*100:.2f}%\")\n",
        "    print(f\"  Final Accuracy:   {disc_final_correct}/{total_questions} = {disc_final_correct/total_questions*100:.2f}%\")\n",
        "    print(f\"  Improvement:      {(disc_final_correct - disc_init_correct)/total_questions*100:+.2f}%\")\n",
        "\n",
        "    overall_improvement = ((gen_final_correct + disc_final_correct) - (gen_init_correct + disc_init_correct)) / (2 * total_questions) * 100\n",
        "    print(f\"\\nOverall Consensus Game Impact: {overall_improvement:+.2f}%\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "#==============================================================================\n",
        "# MAIN EXECUTION\n",
        "#==============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution for baseline consensus game.\"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"BASELINE CONSENSUS GAME\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Simple consensus game between:\")\n",
        "    print(\"  Generator: Phi-2\")\n",
        "    print(\"  Discriminator: Qwen2-1.5B\")\n",
        "    print(\"Fixed parameters, no preprocessing\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Load datasets\n",
        "    arc_df, arc_easy_df = load_arc_datasets()\n",
        "\n",
        "    # Load models\n",
        "    print(\"\\nLoading models...\")\n",
        "    generator_model, generator_tokenizer = load_model(\"microsoft/phi-2\")\n",
        "    discriminator_model, discriminator_tokenizer = load_model(\"Qwen/Qwen2-1.5B-Instruct\")\n",
        "\n",
        "    print(\"\\nModels loaded successfully!\")\n",
        "\n",
        "    # Process ARC Challenge\n",
        "    arc_challenge_results = process_dataset(\n",
        "        generator_model, generator_tokenizer,\n",
        "        discriminator_model, discriminator_tokenizer,\n",
        "        arc_df, \"ARC Challenge\"\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    challenge_filename = \"results/baseline_arc_challenge.csv\"\n",
        "    arc_challenge_results.to_csv(challenge_filename, index=False)\n",
        "    print(f\"\\nARC Challenge results saved to {challenge_filename}\")\n",
        "\n",
        "    # Process ARC Easy\n",
        "    arc_easy_results = process_dataset(\n",
        "        generator_model, generator_tokenizer,\n",
        "        discriminator_model, discriminator_tokenizer,\n",
        "        arc_easy_df, \"ARC Easy\"\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    easy_filename = \"results/baseline_arc_easy.csv\"\n",
        "    arc_easy_results.to_csv(easy_filename, index=False)\n",
        "    print(f\"ARC Easy results saved to {easy_filename}\")\n",
        "\n",
        "    # Cleanup\n",
        "    del generator_model, generator_tokenizer\n",
        "    del discriminator_model, discriminator_tokenizer\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"BASELINE PROCESSING COMPLETE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return arc_challenge_results, arc_easy_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}
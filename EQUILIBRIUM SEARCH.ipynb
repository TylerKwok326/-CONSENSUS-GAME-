{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f63b315-979f-4ace-b520-f9eb44ec204e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ARC-Challenge: 1170 questions\n",
      "Loaded ARC-Easy: 2371 questions\n",
      "Using device: cpu\n",
      "\n",
      "Configuration:\n",
      "  Mode: TEST (Small Subset)\n",
      "  Test Size: 40 questions\n",
      "\n",
      "==================================================\n",
      "LOADING MODELS\n",
      "==================================================\n",
      "Loading dual-model setup:\n",
      "  Generator: microsoft/phi-2\n",
      "  Discriminator: Qwen/Qwen2-1.5B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab6c967c17584e2fbe7145ddc61c20d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully on cpu\n",
      "\n",
      "==================================================\n",
      "TEST MODE\n",
      "==================================================\n",
      "\n",
      "Processing ARC-Challenge subset (20 questions)...\n",
      "Processing 20 questions with Conservative Equilibrium...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 20/20 [34:41<00:00, 104.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ARC-Easy subset (20 questions)...\n",
      "Processing 20 questions with Conservative Equilibrium...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [31:33<00:00, 94.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEST RESULTS\n",
      "==================================================\n",
      "\n",
      "ARC-CHALLENGE TEST RESULTS (20 questions):\n",
      "Generator (Initial):   0.850 (17/20)\n",
      "Discriminator (Initial): 0.600 (12/20)\n",
      "Generator (Final):     0.800 (16/20)\n",
      "Discriminator (Final): 0.200 (4/20)\n",
      "Generator Improvement: -0.050\n",
      "Discriminator Improvement: -0.400\n",
      "Best Method: 0.800\n",
      "\n",
      "ARC-EASY TEST RESULTS (20 questions):\n",
      "Generator (Initial):   0.800 (16/20)\n",
      "Discriminator (Initial): 0.800 (16/20)\n",
      "Generator (Final):     0.800 (16/20)\n",
      "Discriminator (Final): 0.150 (3/20)\n",
      "Generator Improvement: +0.000\n",
      "Discriminator Improvement: -0.650\n",
      "Best Method: 0.800\n",
      "\n",
      " Experiment completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Equilibrium Search for AI Multiple Choice Questions\n",
    "# Uses dual-model approach: Phi-2 (generator) + Qwen2-1.5B (discriminator)\n",
    "# Implements an equilibrium algorithm to improve answer selection\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import google.protobuf\n",
    "import sentencepiece\n",
    "import pandas as pd\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "import gc\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "################################\n",
    "# DATASET LOADING\n",
    "################################\n",
    "\n",
    "def load_arc_datasets():\n",
    "    \"\"\"Load and preprocess ARC-Challenge and ARC-Easy datasets\"\"\"\n",
    "    # Load ARC-Challenge dataset\n",
    "    arc_data = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
    "    arc_df = arc_data.to_pandas()\n",
    "    arc_df = arc_df.drop_duplicates(subset=['question'])\n",
    "    \n",
    "    # Preprocess choices and add subject label\n",
    "    arc_df[\"choices_dic\"] = arc_df[\"choices\"]\n",
    "    arc_df[\"choices\"] = arc_df[\"choices\"].apply(lambda x: x[\"text\"])\n",
    "    arc_df[\"subject\"] = \"science\"\n",
    "    \n",
    "    # Load ARC-Easy dataset with same preprocessing\n",
    "    arc_data_easy = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\", split=\"test\")\n",
    "    arc_df_easy = arc_data_easy.to_pandas()\n",
    "    arc_df_easy = arc_df_easy.drop_duplicates(subset=['question'])\n",
    "    arc_df_easy[\"choices_dic\"] = arc_df_easy[\"choices\"]\n",
    "    arc_df_easy[\"choices\"] = arc_df_easy[\"choices\"].apply(lambda x: x[\"text\"])\n",
    "    arc_df_easy[\"subject\"] = \"science\"\n",
    "    \n",
    "    print(f\"Loaded ARC-Challenge: {arc_df.shape[0]} questions\")\n",
    "    print(f\"Loaded ARC-Easy: {arc_df_easy.shape[0]} questions\")\n",
    "    \n",
    "    return arc_df, arc_df_easy\n",
    "\n",
    "# Load datasets\n",
    "arc_df, arc_df_easy = load_arc_datasets()\n",
    "\n",
    "# Setup device configuration\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    device = \"cuda\"\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Import required transformers components\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList, MinLengthLogitsProcessor\n",
    "import accelerate\n",
    "import re\n",
    "\n",
    "################################\n",
    "# CONFIGURATION\n",
    "################################\n",
    "\n",
    "# Processing mode: True for full dataset, False for testing with small subset\n",
    "USE_BATCH_PROCESSING = False  # Changed to False for quick testing\n",
    "BATCH_SIZE = 50              # Questions per batch (approximately 1.4 hours per batch)\n",
    "TEST_SIZE_CHALLENGE = 20     # Test mode: subset size for ARC-Challenge (increased for better results)\n",
    "TEST_SIZE_EASY = 20          # Test mode: subset size for ARC-Easy (increased for better results)\n",
    "\n",
    "################################\n",
    "# GENERATOR FUNCTIONS (PHI-2)\n",
    "################################\n",
    "\n",
    "def format_subject(subject):\n",
    "    \"\"\"Convert underscore-separated subject to readable format\"\"\"\n",
    "    return \" \".join(subject.split(\"_\"))\n",
    "\n",
    "def build_generator_prompt(subject, target_question, target_choices, get_correct):\n",
    "    \"\"\"\n",
    "    Build prompt for generator model (Phi-2) to predict correct/incorrect answers\n",
    "    \n",
    "    Args:\n",
    "        subject: Subject area (e.g., \"science\")\n",
    "        target_question: The question text\n",
    "        target_choices: List of answer choices\n",
    "        get_correct: If True, asks for correct answer; if False, asks for incorrect answer\n",
    "    \"\"\"\n",
    "    prompt = f\"The following are multiple choice questions (with answers) about {format_subject(subject)}.\\n\\n\"\n",
    "    prompt += f\"{target_question}\"\n",
    "    \n",
    "    # Add labeled choices (A, B, C, D)\n",
    "    for i, choice in enumerate(target_choices):\n",
    "        prompt += f\"\\n{chr(65+i)}. {choice}\"\n",
    "    \n",
    "    # Set prompt ending based on what we want to elicit\n",
    "    prompt += \"\\nAnswer:\" if get_correct else \"\\nIncorrect Answer:\"\n",
    "    return prompt\n",
    "\n",
    "def get_generator_answer_probs(model, tokenizer, prompt_text, choices_list):\n",
    "    \"\"\"\n",
    "    Get probability distribution over answer choices from generator model\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping from choice letters (A, B, C, D) to probabilities\n",
    "    \"\"\"\n",
    "    # Tokenize prompt and get model predictions\n",
    "    input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get logits for next token prediction\n",
    "        logits = model(input_ids=input_ids).logits[0, -1]\n",
    "\n",
    "    # Extract logits for each answer choice letter\n",
    "    choices = [f\"{chr(65+i)}\" for i, _ in enumerate(choices_list)]\n",
    "    choice_logits = []\n",
    "    \n",
    "    for letter in choices:\n",
    "        try:\n",
    "            token_id = tokenizer.encode(letter, add_special_tokens=False)\n",
    "            if token_id:\n",
    "                choice_logits.append(logits[token_id[0]].item())\n",
    "            else:\n",
    "                choice_logits.append(-100.0)  # Very low probability if token not found\n",
    "        except:\n",
    "            choice_logits.append(-100.0)\n",
    "    \n",
    "    # Convert logits to probabilities using softmax\n",
    "    choice_logits = torch.tensor(choice_logits, device=model.device).float()\n",
    "    probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
    "    \n",
    "    return {choice: prob for choice, prob in zip(choices, probs)}\n",
    "\n",
    "def generator_probs(subject, question, choices_list, get_correct, model, tokenizer):\n",
    "    \"\"\"Wrapper function to get generator probabilities for a question\"\"\"\n",
    "    choices = [f\"{chr(65+i)}. {choice}\" for i, choice in enumerate(choices_list)]\n",
    "    prompt = build_generator_prompt(subject, question, choices, get_correct)\n",
    "    return get_generator_answer_probs(model, tokenizer, prompt, choices_list)\n",
    "\n",
    "def get_initial_generator_probs(row, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Get initial generator probabilities for both correct and incorrect answer modes\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains 'correct' and 'incorrect' probability distributions\n",
    "    \"\"\"\n",
    "    gen_init = {\"correct\": {}, \"incorrect\": {}}\n",
    "    subject, question, choices = row[\"subject\"], row[\"question\"], row[\"choices\"]\n",
    "    \n",
    "    # Get probabilities for both correct and incorrect answer prompts\n",
    "    for get_correct in [True, False]:\n",
    "        choice_probs = generator_probs(subject, question, choices, get_correct, model, tokenizer)\n",
    "        key = \"correct\" if get_correct else \"incorrect\"\n",
    "        gen_init[key] = choice_probs\n",
    "    \n",
    "    return gen_init\n",
    "\n",
    "################################\n",
    "# DISCRIMINATOR FUNCTIONS (QWEN2-1.5B)\n",
    "################################\n",
    "\n",
    "def build_discriminator_prompt(subject, question, proposed_answer):\n",
    "    \"\"\"\n",
    "    Build prompt for discriminator model to evaluate answer correctness\n",
    "    \n",
    "    Args:\n",
    "        subject: Subject area\n",
    "        question: The question text  \n",
    "        proposed_answer: Answer choice to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted prompt asking for correctness judgment\n",
    "    \"\"\"\n",
    "    return f\"\"\"You are an expert evaluator of questions about {format_subject(subject)}. \n",
    "Determine if the proposed answer is correct. Output ONLY 'A' or 'B'.\n",
    "Question: {question}\n",
    "Proposed Answer: {proposed_answer}\n",
    "\n",
    "Is this answer correct? Respond ONLY with:\n",
    "A. Correct\n",
    "B. Incorrect\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def get_discriminator_probs(model, tokenizer, prompt_text, choices_list):\n",
    "    \"\"\"\n",
    "    Get correctness probabilities from discriminator model\n",
    "    \n",
    "    Returns:\n",
    "        dict: {'correct': prob, 'incorrect': prob}\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids).logits[0, -1]\n",
    "\n",
    "    try:\n",
    "        # Get logits for 'A' (correct) and 'B' (incorrect) tokens\n",
    "        choice_logits = torch.tensor([\n",
    "            logits[tokenizer(\"A\").input_ids[-1]],\n",
    "            logits[tokenizer(\"B\").input_ids[-1]],\n",
    "        ]).float()\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
    "        return {\"correct\": probs[0], \"incorrect\": probs[1]}\n",
    "    except:\n",
    "        # Fallback to uniform distribution if token extraction fails\n",
    "        return {\"correct\": 0.5, \"incorrect\": 0.5}\n",
    "\n",
    "def evaluate_answer_correctness(row, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Evaluate correctness of each answer choice using discriminator model\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping from choice letters to correctness probabilities\n",
    "    \"\"\"\n",
    "    subject = row[\"subject\"]\n",
    "    question = row[\"question\"]\n",
    "    choices = row[\"choices\"]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Evaluate each answer choice\n",
    "    for idx, answer in enumerate(choices):\n",
    "        prompt = build_discriminator_prompt(subject, question, answer)\n",
    "        probs = get_discriminator_probs(model, tokenizer, prompt, choices)\n",
    "        choice_letter = chr(65 + idx)  # Convert index to letter (A, B, C, D)\n",
    "        results[choice_letter] = probs\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_initial_discriminator_probs(row, model, tokenizer):\n",
    "    \"\"\"Get initial discriminator probabilities for all answer choices\"\"\"\n",
    "    return evaluate_answer_correctness(row, model, tokenizer)\n",
    "\n",
    "################################\n",
    "# EQUILIBRIUM SEARCH ALGORITHM\n",
    "################################\n",
    "\n",
    "def pick_answer(gen, disc, candidates, method=\"generator\"):\n",
    "    \"\"\"\n",
    "    Select best answer based on either generator or discriminator probabilities\n",
    "    \n",
    "    Args:\n",
    "        gen: Generator probability distributions\n",
    "        disc: Discriminator probability distributions  \n",
    "        candidates: List of answer choice letters\n",
    "        method: Either \"generator\" or \"discriminator\"\n",
    "    \n",
    "    Returns:\n",
    "        str: Best answer choice letter\n",
    "    \"\"\"\n",
    "    if method == \"generator\":\n",
    "        # Choose answer with highest generator \"correct\" probability\n",
    "        return max(candidates, key=lambda y: gen[\"correct\"][y])\n",
    "    else:\n",
    "        # Choose answer with highest discriminator \"correct\" probability\n",
    "        return max(candidates, key=lambda y: disc[y][\"correct\"])\n",
    "\n",
    "def softmax(arr):\n",
    "    \"\"\"Numerically stable softmax implementation\"\"\"\n",
    "    m = np.max(arr)\n",
    "    exp_vals = np.exp(arr - m)\n",
    "    return exp_vals / np.sum(exp_vals)\n",
    "\n",
    "def equilibrium_search(gen_init, disc_init, candidates, \n",
    "                       T=5,           \n",
    "                       eta_G=0.1,      \n",
    "                       eta_D=0.1,     # Normal discriminator learning rate\n",
    "                       lam_G=0.1,     # Strong generator regularization\n",
    "                       lam_D=0.1):    # Normal discriminator regularization\n",
    "    \"\"\"\n",
    "    Equilibrium Search Algorithm\n",
    "    \n",
    "    This algorithm iteratively updates generator and discriminator policies to reach\n",
    "    an equilibrium while preserving the generator's strong initial performance.\n",
    "    \n",
    "    This prevents the generator from deviating too much from its strong initial policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize working copies of probability distributions\n",
    "    gen = {\"correct\": dict(gen_init[\"correct\"]), \n",
    "           \"incorrect\": dict(gen_init[\"incorrect\"])}\n",
    "    disc = {y: dict(disc_init[y]) for y in candidates}\n",
    "\n",
    "    # Initialize Q-values (cumulative rewards) for both models\n",
    "    Qg = {\"correct\": {y: 0.0 for y in candidates}, \n",
    "          \"incorrect\": {y: 0.0 for y in candidates}}\n",
    "    Qd = {y: {\"correct\": 0.0, \"incorrect\": 0.0} for y in candidates}\n",
    "\n",
    "    # Iterative equilibrium search\n",
    "    for t in range(1, T + 1):\n",
    "        # Update Q-values based on current policies\n",
    "        # Generator Q-values incorporate discriminator feedback\n",
    "        for v in [\"correct\", \"incorrect\"]:\n",
    "            for y in candidates:\n",
    "                Qg[v][y] += (1.0 / (2.0 * t)) * disc[y][v]\n",
    "\n",
    "        # Discriminator Q-values incorporate generator feedback  \n",
    "        for y in candidates:\n",
    "            for v in [\"correct\", \"incorrect\"]:\n",
    "                Qd[y][v] += (1.0 / (2.0 * t)) * gen[v][y]\n",
    "\n",
    "        # Update generator policy with regularization\n",
    "        # Strong regularization keeps it close to initial policy\n",
    "        for v in [\"correct\", \"incorrect\"]:\n",
    "            logits = []\n",
    "            for y in candidates:\n",
    "                # Combine Q-value with regularization toward initial policy\n",
    "                val = (Qg[v][y] + lam_G * math.log(gen_init[v][y] + 1e-12)) / (1/eta_G + lam_G)\n",
    "                logits.append(val)\n",
    "\n",
    "            # Apply softmax to get new probability distribution\n",
    "            new_probs = softmax(np.array(logits))\n",
    "            for i, y in enumerate(candidates):\n",
    "                gen[v][y] = new_probs[i]\n",
    "        \n",
    "        # Update discriminator policy\n",
    "        # Less regularization allows more adaptation\n",
    "        for y in candidates:\n",
    "            logits_correct = []\n",
    "            logits_incorrect = []\n",
    "            \n",
    "            for y_inner in candidates:\n",
    "                # Correct probabilities\n",
    "                val_correct = (Qd[y][\"correct\"] + lam_D * math.log(disc_init[y][\"correct\"] + 1e-12)) / (1/eta_D + lam_D)\n",
    "                logits_correct.append(val_correct if y_inner == y else -float('inf'))\n",
    "                \n",
    "                # Incorrect probabilities  \n",
    "                val_incorrect = (Qd[y][\"incorrect\"] + lam_D * math.log(disc_init[y][\"incorrect\"] + 1e-12)) / (1/eta_D + lam_D)\n",
    "                logits_incorrect.append(val_incorrect if y_inner == y else -float('inf'))\n",
    "\n",
    "            # Update this answer choice's probabilities\n",
    "            new_probs_correct = softmax(np.array([val_correct]))\n",
    "            new_probs_incorrect = softmax(np.array([val_incorrect]))\n",
    "            \n",
    "            disc[y][\"correct\"] = new_probs_correct[0]\n",
    "            disc[y][\"incorrect\"] = new_probs_incorrect[0]\n",
    "\n",
    "    return gen, disc\n",
    "\n",
    "################################\n",
    "# MODEL LOADING\n",
    "################################\n",
    "\n",
    "def load_dual_models():\n",
    "    \"\"\"\n",
    "    Load the dual-model setup: Phi-2 (generator) + Qwen2-1.5B (discriminator)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (generator_model, generator_tokenizer, discriminator_model, discriminator_tokenizer)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading dual-model setup:\")\n",
    "    print(\"  Generator: microsoft/phi-2\") \n",
    "    print(\"  Discriminator: Qwen/Qwen2-1.5B-Instruct\")\n",
    "    \n",
    "    # Configure device and precision\n",
    "    if device == \"cuda\":\n",
    "        torch_dtype = torch.float16\n",
    "        device_map = \"cuda\"\n",
    "    else:\n",
    "        torch_dtype = torch.float32\n",
    "        device_map = \"cpu\"\n",
    "    \n",
    "    # Load Phi-2 as generator\n",
    "    generator_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"microsoft/phi-2\",\n",
    "        torch_dtype=torch_dtype,\n",
    "        load_in_8bit=False,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    generator_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "    if generator_tokenizer.pad_token is None:\n",
    "        generator_tokenizer.pad_token = generator_tokenizer.eos_token\n",
    "    \n",
    "    # Load Qwen2-1.5B as discriminator\n",
    "    discriminator_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "        torch_dtype=torch_dtype,\n",
    "        load_in_8bit=False,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    discriminator_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-1.5B-Instruct\", trust_remote_code=True)\n",
    "    if discriminator_tokenizer.pad_token is None:\n",
    "        discriminator_tokenizer.pad_token = discriminator_tokenizer.eos_token\n",
    "    \n",
    "    print(f\"Models loaded successfully on {device}\")\n",
    "    \n",
    "    return generator_model, generator_tokenizer, discriminator_model, discriminator_tokenizer\n",
    "\n",
    "################################\n",
    "# MAIN PROCESSING PIPELINE\n",
    "################################\n",
    "\n",
    "def process_questions_with_equilibrium(generator_model, generator_tokenizer, \n",
    "                                      discriminator_model, discriminator_tokenizer, df):\n",
    "    \"\"\"\n",
    "    Main pipeline for processing questions using equilibrium search\n",
    "    \n",
    "    Args:\n",
    "        generator_model, generator_tokenizer: Phi-2 model and tokenizer\n",
    "        discriminator_model, discriminator_tokenizer: Qwen2-1.5B model and tokenizer\n",
    "        df: DataFrame containing questions to process\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (processed_dataframe, average_time_per_question)\n",
    "    \"\"\"\n",
    "    \n",
    "    category_df = df.copy()\n",
    "    \n",
    "    # Initialize result lists\n",
    "    gen_answer = []\n",
    "    disc_answer = []\n",
    "    gen_init_answer = []\n",
    "    disc_init_answer = []\n",
    "    disc_init_policy = []\n",
    "    gen_init_policy = []\n",
    "    disc_final_policy_consensus = []\n",
    "    gen_final_policy_consensus = []\n",
    "    \n",
    "    print(f\"Processing {len(category_df)} questions with Equilibrium Search...\")\n",
    "    \n",
    "    question_times = []\n",
    "    \n",
    "    # Process each question\n",
    "    for idx, (_, row) in enumerate(tqdm(category_df.iterrows(), total=len(category_df))):\n",
    "        question_start_time = time.time()\n",
    "\n",
    "        # Get initial discriminator probabilities\n",
    "        disc_init = get_initial_discriminator_probs(row, discriminator_model, discriminator_tokenizer)\n",
    "        disc_init_policy.append(disc_init)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        # Get initial generator probabilities  \n",
    "        gen_init = get_initial_generator_probs(row, generator_model, generator_tokenizer)\n",
    "        gen_init_policy.append(gen_init)\n",
    "        \n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Record initial answers (before equilibrium search)\n",
    "        gen_init_answer.append(max(gen_init[\"correct\"], key=gen_init[\"correct\"].get))\n",
    "        disc_init_answer.append(max(disc_init, key=lambda choice: disc_init[choice][\"correct\"]))\n",
    "        \n",
    "        # Prepare for equilibrium search\n",
    "        candidates = [f\"{chr(65+i)}\" for i in range(len(row[\"choices\"]))]\n",
    "\n",
    "        # Run equilibrium search\n",
    "        gen_final, disc_final = equilibrium_search(\n",
    "            gen_init, disc_init, candidates,\n",
    "            T=5,        # Fewer iterations  \n",
    "            eta_G=0.1,  # Slow generator learning\n",
    "            eta_D=0.1,  # Normal discriminator learning\n",
    "            lam_G=0.5,  # Strong generator regularization\n",
    "            lam_D=0.1   # Normal discriminator regularization\n",
    "        )\n",
    "        \n",
    "        # Store final policies\n",
    "        disc_final_policy_consensus.append(disc_final)\n",
    "        gen_final_policy_consensus.append(gen_final)\n",
    "\n",
    "        # Select final answers using updated policies\n",
    "        best_answer_g = pick_answer(gen_final, disc_final, candidates, method=\"generator\")\n",
    "        best_answer_d = pick_answer(gen_final, disc_final, candidates, method=\"discriminator\")\n",
    "        \n",
    "        gen_answer.append(best_answer_g)\n",
    "        disc_answer.append(best_answer_d)\n",
    "        \n",
    "        # Track timing\n",
    "        question_time = time.time() - question_start_time\n",
    "        question_times.append(question_time)\n",
    "    \n",
    "    avg_time_per_question = np.mean(question_times)\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    category_df[\"gen_init_answer\"] = gen_init_answer\n",
    "    category_df[\"disc_answer\"] = disc_answer\n",
    "    category_df[\"gen_answer\"] = gen_answer\n",
    "    category_df[\"disc_init_answer\"] = disc_init_answer\n",
    "    category_df[\"disc_final_policy_consensus\"] = disc_final_policy_consensus\n",
    "    category_df[\"disc_init_policy\"] = disc_init_policy\n",
    "    category_df[\"gen_init_policy\"] = gen_init_policy\n",
    "    category_df[\"gen_final_policy_consensus\"] = gen_final_policy_consensus\n",
    "    \n",
    "    return category_df, avg_time_per_question\n",
    "\n",
    "################################\n",
    "# BATCH PROCESSING\n",
    "################################\n",
    "\n",
    "def process_in_batches(generator_model, generator_tokenizer, \n",
    "                      discriminator_model, discriminator_tokenizer, \n",
    "                      df, dataset_name, batch_size=50):\n",
    "    \"\"\"\n",
    "    Process large dataset in batches to manage memory and save intermediate results\n",
    "    \n",
    "    Args:\n",
    "        models and tokenizers: The loaded models\n",
    "        df: Dataset to process\n",
    "        dataset_name: Name for saving files\n",
    "        batch_size: Number of questions per batch\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Complete processed results\n",
    "    \"\"\"\n",
    "    \n",
    "    total_questions = len(df)\n",
    "    num_batches = (total_questions + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"\\nProcessing {dataset_name} in {num_batches} batches of {batch_size}\")\n",
    "    print(f\"Total questions: {total_questions}\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, total_questions)\n",
    "        batch_df = df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        print(f\"\\nBatch {batch_idx + 1}/{num_batches}: Questions {start_idx+1}-{end_idx}\")\n",
    "        \n",
    "        batch_start_time = time.time()\n",
    "        batch_result, avg_time = process_questions_with_equilibrium(\n",
    "            generator_model, generator_tokenizer,\n",
    "            discriminator_model, discriminator_tokenizer, \n",
    "            batch_df\n",
    "        )\n",
    "        batch_time = time.time() - batch_start_time\n",
    "        \n",
    "        # Save batch result immediately for crash recovery\n",
    "        batch_filename = f'Data/{dataset_name}_batch_{batch_idx+1}_of_{num_batches}.csv'\n",
    "        batch_result.to_csv(batch_filename, index=False)\n",
    "        all_results.append(batch_result)\n",
    "        \n",
    "        # Progress reporting\n",
    "        completed_questions = end_idx\n",
    "        remaining_questions = total_questions - completed_questions\n",
    "        if remaining_questions > 0:\n",
    "            estimated_remaining_time = (remaining_questions / len(batch_df)) * batch_time\n",
    "            print(f\"Batch complete: {batch_time/60:.1f}min | Remaining: {estimated_remaining_time/3600:.1f}h\")\n",
    "        else:\n",
    "            print(f\"Batch complete: {batch_time/60:.1f}min\")\n",
    "        \n",
    "        # Memory cleanup between batches\n",
    "        gc.collect()\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Combine all batch results\n",
    "    print(f\"\\nCombining {num_batches} batches...\")\n",
    "    final_result = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Save final combined result\n",
    "    final_filename = f'Data/{dataset_name}_complete_results.csv'\n",
    "    final_result.to_csv(final_filename, index=False)\n",
    "    print(f\"Results saved: {final_filename}\")\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "################################\n",
    "# RESULTS ANALYSIS\n",
    "################################\n",
    "\n",
    "def analyze_results(results_df, dataset_name):\n",
    "    \"\"\"Analyze and display results for a processed dataset\"\"\"\n",
    "    total_questions = len(results_df)\n",
    "    \n",
    "    # Calculate accuracy metrics\n",
    "    gen_init_acc = (results_df['gen_init_answer'] == results_df['answerKey']).mean()\n",
    "    disc_init_acc = (results_df['disc_init_answer'] == results_df['answerKey']).mean()\n",
    "    gen_final_acc = (results_df['gen_answer'] == results_df['answerKey']).mean()\n",
    "    disc_final_acc = (results_df['disc_answer'] == results_df['answerKey']).mean()\n",
    "    \n",
    "    print(f\"\\n{dataset_name.upper()} RESULTS ({total_questions} questions):\")\n",
    "    print(f\"Generator (Initial):   {gen_init_acc:.3f} ({int(gen_init_acc*total_questions)}/{total_questions})\")\n",
    "    print(f\"Discriminator (Initial): {disc_init_acc:.3f} ({int(disc_init_acc*total_questions)}/{total_questions})\")\n",
    "    print(f\"Generator (Final):     {gen_final_acc:.3f} ({int(gen_final_acc*total_questions)}/{total_questions})\")\n",
    "    print(f\"Discriminator (Final): {disc_final_acc:.3f} ({int(disc_final_acc*total_questions)}/{total_questions})\")\n",
    "    print(f\"Generator Improvement: {gen_final_acc - gen_init_acc:+.3f}\")\n",
    "    print(f\"Discriminator Improvement: {disc_final_acc - disc_init_acc:+.3f}\")\n",
    "    print(f\"Best Method: {max(gen_final_acc, disc_final_acc):.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'gen_init': gen_init_acc,\n",
    "        'disc_init': disc_init_acc, \n",
    "        'gen_final': gen_final_acc,\n",
    "        'disc_final': disc_final_acc\n",
    "    }\n",
    "\n",
    "################################\n",
    "# MAIN EXECUTION\n",
    "################################\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    if USE_BATCH_PROCESSING:\n",
    "        print(f\"  Mode: BATCH PROCESSING (Full Dataset)\")\n",
    "        print(f\"  Batch Size: {BATCH_SIZE} questions\")\n",
    "        total_estimated_batches = (len(arc_df) + BATCH_SIZE - 1) // BATCH_SIZE + (len(arc_df_easy) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "        print(f\"  Estimated Total Batches: {total_estimated_batches}\")\n",
    "    else:\n",
    "        print(f\"  Mode: TEST (Small Subset)\")\n",
    "        print(f\"  Test Size: {TEST_SIZE_CHALLENGE + TEST_SIZE_EASY} questions\")\n",
    "\n",
    "    # Clean up any existing models and load fresh ones\n",
    "    gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Load models\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"LOADING MODELS\")\n",
    "    print(\"=\"*50)\n",
    "    generator_model, generator_tokenizer, discriminator_model, discriminator_tokenizer = load_dual_models()\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs('Data', exist_ok=True)\n",
    "\n",
    "    if USE_BATCH_PROCESSING:\n",
    "        # Full dataset processing mode\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BATCH PROCESSING - FULL DATASET\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Process ARC-Challenge\n",
    "        challenge_results = process_in_batches(\n",
    "            generator_model, generator_tokenizer,\n",
    "            discriminator_model, discriminator_tokenizer,\n",
    "            arc_df, \"arc_challenge\", BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        # Process ARC-Easy\n",
    "        easy_results = process_in_batches(\n",
    "            generator_model, generator_tokenizer,\n",
    "            discriminator_model, discriminator_tokenizer,\n",
    "            arc_df_easy, \"arc_easy\", BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        # Analyze final results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"FINAL RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        challenge_metrics = analyze_results(challenge_results, \"ARC-Challenge\")\n",
    "        easy_metrics = analyze_results(easy_results, \"ARC-Easy\")\n",
    "        \n",
    "        print(f\"\\nSUMMARY:\")\n",
    "        print(f\"ARC-Challenge Best: {max(challenge_metrics['gen_final'], challenge_metrics['disc_final']):.1%}\")\n",
    "        print(f\"ARC-Easy Best: {max(easy_metrics['gen_final'], easy_metrics['disc_final']):.1%}\")\n",
    "\n",
    "    else:\n",
    "        # Test mode with small subsets\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TEST MODE\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Create test subsets\n",
    "        arc_df_test = arc_df.head(TEST_SIZE_CHALLENGE).copy()\n",
    "        arc_df_easy_test = arc_df_easy.head(TEST_SIZE_EASY).copy()\n",
    "\n",
    "        # Process test subsets\n",
    "        print(f\"\\nProcessing ARC-Challenge subset ({TEST_SIZE_CHALLENGE} questions)...\")\n",
    "        challenge_results, _ = process_questions_with_equilibrium(\n",
    "            generator_model, generator_tokenizer,\n",
    "            discriminator_model, discriminator_tokenizer, \n",
    "            arc_df_test\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nProcessing ARC-Easy subset ({TEST_SIZE_EASY} questions)...\")\n",
    "        easy_results, _ = process_questions_with_equilibrium(\n",
    "            generator_model, generator_tokenizer,\n",
    "            discriminator_model, discriminator_tokenizer,\n",
    "            arc_df_easy_test\n",
    "        )\n",
    "\n",
    "        # Save test results\n",
    "        challenge_results.to_csv(f'Data/arc_challenge_test_{TEST_SIZE_CHALLENGE}.csv', index=False)\n",
    "        easy_results.to_csv(f'Data/arc_easy_test_{TEST_SIZE_EASY}.csv', index=False)\n",
    "\n",
    "        # Analyze test results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TEST RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        analyze_results(challenge_results, \"ARC-Challenge Test\")\n",
    "        analyze_results(easy_results, \"ARC-Easy Test\")\n",
    "\n",
    "    # Final cleanup\n",
    "    gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n Experiment completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ff757-8862-4642-9ea9-fdab14be3b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

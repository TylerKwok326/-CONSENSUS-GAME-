# CONSENSUS-GAME

## Overview
This code tests Phi-2 and Qwen2 on 50 calibration questions from the ARC dataset and evaluates four aspects of model performance:
- Accuracy – how often predictions are correct.
- Confidence – average probability assigned to chosen answers.
- Consistency – predictability of answer patterns, measured by entropy.
- Robustness – confidence restricted to correct answers.
These metrics combine into a single base strength score used to weight each model during adversarial training. Calibration questions are removed from testing to prevent data leakage.
## Workflow
Preprocessing: Filter out low-confidence answers (<10%), shuffle remaining choices to avoid position bias.

### Adversarial Game:
- Generator (Phi-2) proposes answers.
- Discriminator (Qwen2) evaluates correctness.
- Both update strategies iteratively using weighted softmax and Q-learning.
- Temperature Scheduling: Adjusts exploration vs. exploitation depending on strategy stability.
- Stopping Criteria: Consensus reached when (a) both models >80% confidence in top choice, (b) max 5 iterations, or (c) strategy change <0.001.
## Results
- Baseline: Generator ~73.4%, Discriminator ~60.9%.
### After weighted equilibrium search:
- Generator: 75.0% (+1.6%)
- Discriminator: 68.0% (+7.1%)
- Overall accuracy: 73.6%

## Methodology 

The code begins by testing Phi-2 and Qwen2 on 50 sample questions from the ARC dataset. It measures four key aspects: (1) how often each model answers correctly, (2) how confident they are in their predictions, (3) how consistent their answers are across similar questions, and (4) whether they show high confidence specifically when they are correct. (1) Accuracy is calculated by looping through 50 test questions, collecting each model's prediction, and checking if it matches the correct answer. (2) Confidence is measured as the average probability score the model assigns to its chosen answers. (3) Consistency is quantified using entropy, which measures the predictability of answer patterns. For example, if a model always selects option A, entropy is zero and consistency is 1.0. If it distributes answers evenly across A, B, C, and D, entropy is maximal and consistency drops to 0.0. Intermediate distributions, such as 40\% A, 30\% B, 20\% C, and 10\% D, yield consistency values around 0.6. (4) Robustness is calculated as the model's confidence restricted to correct answers only. These four metrics are then combined into a single "base strength" score. The 50 calibration questions are removed from the test dataset to avoid data leakage. When processing each new question, the system first runs a preprocessing step. Both models analyze all answer choices and assign confidence scores. Any choice receiving less than 10\% probability from the combined models gets filtered out. The remaining choices are randomly shuffled to prevent position bias, with the system maintaining a position mapping to correctly track the original answer key through this transformation. If only one choice survives this filtering, the system achieves "consensus" and immediately selects that answer without any further processing. For questions with multiple choices, the system prepares for the adversarial game by gathering initial probabilities. Phi-2 (as generator) produces two probability distributions - one for generating correct answers and one for incorrect answers across all choices. Qwen2 (as discriminator) evaluates each answer choice and assigns probabilities for whether it's correct or incorrect. These initial distributions become the starting strategies for the game. The system then uses two different softmax operations that serve distinct purposes throughout the equilibrium search: The first softmax converts base strengths into dynamic weights using the formula: weights = softmax(base\_strengths / temperature). This happens once per iteration, transforming the calibrated strengths into normalized weights that sum to 1.0. Temperature works like a dial - at high values, both models get nearly equal say regardless of their base strengths. At low values, the stronger model dominates. The adaptive temperature strategy specifically adjusts based on the game state: starting at base\_temp × 1.5 for the first two iterations to encourage exploration, then monitoring strategy stability. When both models' strategies become very stable (>95\% stability), temperature drops to base\_temp × 0.7 to exploit the emerging consensus. When strategies are unstable (<80\% stability), temperature increases to base\_temp × 1.3 to explore more options. These weights determine how much each model's feedback counts in the current iteration. During the equilibrium search, these weights control how much each model influences the process through a Q-learning mechanism. The system maintains Q-values that accumulate weighted feedback over iterations: Qg[v][y] += (disc\_weight / (2.0 * t)) * disc[y][v] for the generator and similarly for the discriminator. Importantly, the learning rates themselves are also weighted: eta\_G = 0.1 * gen\_weight and eta\_D = 0.1 * disc\_weight, meaning stronger models not only have more influence but also adapt their strategies faster. The second softmax updates each model's strategy by converting these Q-values into probability distributions. For the generator choosing which answer to propose: new\_probs = softmax((Q\_values + lambda * log(initial\_probs)) / (1/learning\_rate + lambda)). The lambda parameters (0.1 for generator, 0.01 for discriminator) provide regularization, preventing strategies from deviating too far from their initial beliefs. This happens multiple times per iteration - updating how likely the generator is to propose each answer as correct/incorrect, and how likely the discriminator is to judge each answer as correct/incorrect. The generator (Phi-2) proposes answers while the discriminator (Qwen2) judges them. But here's the key part: when the discriminator gives feedback to the generator, that feedback is multiplied by the discriminator's current weight. If calibration showed Qwen2 is weaker, its judgments count less. The same happens in reverse - the generator's proposals influence the discriminator proportionally to the generator's weight. This creates a weighted learning process where each model's influence matches its proven ability. The models iterate back and forth, adjusting their strategies based on weighted feedback from each other. The process stops when convergence is achieved through one of three criteria: (1) both models concentrate more than 80\% probability on their top choice, (2) the maximum 5 iterations are reached, or (3) the total change in strategies between iterations falls below 0.001 (early stopping). The final answer combines both models' opinions using their final weights. From our baseline result, while using Phi-2 as generator and Qwen2 as the discriminator we see that there is a trade-off pattern. Discriminator accuracy improved by \~12\% and the Generator accuracy declined slightly by ~1.5%. After the implementation we added, we achieved simultaneous improvement, where the Generator accuracy increased from 73.42\% to 75\% (+1.58\%) and the Discriminator accuracy increased from 60.85\% to 68\% (+7.15\%). Resulted in the highest overall accuracy of 73.6%."

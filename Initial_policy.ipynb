{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iIZM-prCJnV"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import google.protobuf\n",
        "import sentencepiece\n",
        "import pandas as pd\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset\n",
        "import gc\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"hf_***REDACTED***\"\n",
        "login(os.environ[\"HF_TOKEN\"])\n",
        "hf_token = os.environ.get('HF_TOKEN')\n",
        "\n",
        "arc_data = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
        "arc_df = arc_data.to_pandas()\n",
        "arc_df = arc_df.drop_duplicates(subset=['question'])\n",
        "print(arc_df.shape)\n",
        "print(arc_df.columns)\n",
        "\n",
        "arc_df[\"choices_dic\"] = arc_df[\"choices\"]\n",
        "arc_df[\"choices\"] = arc_df[\"choices\"].apply(lambda x: x[\"text\"])\n",
        "arc_df[\"subject\"] = \"science\"\n",
        "arc_df.head(2)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList, MinLengthLogitsProcessor\n",
        "import accelerate\n",
        "print(accelerate.__version__)\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
        "\n",
        "import re\n",
        "import math\n",
        "\n",
        "\n",
        "def format_subject(subject):\n",
        "    l = subject.split(\"_\")\n",
        "    s = \"\"\n",
        "    for entry in l:\n",
        "        s += \" \" + entry\n",
        "    return s\n",
        "\n",
        "\n",
        "def build_generator_prompt(\n",
        "    subject,\n",
        "    target_question,\n",
        "    target_choices,\n",
        "    get_correct\n",
        "):\n",
        "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(\n",
        "        format_subject(subject))\n",
        "\n",
        "    prompt += f\"{target_question}\"\n",
        "    for i, c in enumerate(target_choices):\n",
        "        prompt += \"\\n{}\".format(c)\n",
        "\n",
        "    if get_correct:\n",
        "        prompt += \"\\nAnswer:\"\n",
        "    else:\n",
        "        prompt += \"\\nIncorrect Answer:\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def get_generator_answer_probs(model, tokenizer, prompt_text, choices_list):\n",
        "    input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    logits = model(input_ids=input_ids).logits[0, -1]\n",
        "\n",
        "    choices = [f\"{chr(65+i)}\" for i, choice in enumerate(choices_list)]\n",
        "    choice_logits = []\n",
        "    for letter in choices:\n",
        "        token_id = tokenizer(letter, return_tensors=\"pt\").input_ids[0, -1].item()\n",
        "        choice_logits.append(logits[token_id].item())\n",
        "\n",
        "    choice_logits = torch.tensor(choice_logits, device=model.device).float()\n",
        "    probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
        "\n",
        "    choice_probs = {choice: prob for choice, prob in zip(choices, probs)}\n",
        "\n",
        "    return choice_probs\n",
        "\n",
        "\n",
        "def generator_probs(subject, question, choices_list, get_correct, model, tokenizer):\n",
        "    choices = [f\"{chr(65+i)}. {choice}\" for i, choice in enumerate(choices_list)]\n",
        "\n",
        "    prompt = build_generator_prompt(subject, question, choices, get_correct)\n",
        "\n",
        "    probs = get_generator_answer_probs(model, tokenizer, prompt, choices_list)\n",
        "\n",
        "    return probs\n",
        "\n",
        "\n",
        "def get_initial_generator_probs(row, model, tokenizer):\n",
        "    gen_init = {\"correct\": {}, \"incorrect\": {}}\n",
        "    x, y_list, subject = row[\"question\"], row[\"choices\"], row[\"subject\"]\n",
        "    for v in [True, False]:\n",
        "        choices_letter_prob = generator_probs(subject, x, y_list, v, model, tokenizer)\n",
        "        if v:\n",
        "            for key, val in choices_letter_prob.items():\n",
        "                gen_init[\"correct\"][key] = val\n",
        "        else:\n",
        "            for key, val in choices_letter_prob.items():\n",
        "                gen_init[\"incorrect\"][key] = val\n",
        "\n",
        "    return gen_init\n",
        "\n",
        "\n",
        "def build_discriminator_prompt(\n",
        "    subject: str,\n",
        "    question: str,\n",
        "    proposed_answer: str\n",
        ") -> str:\n",
        "    prompt = f\"\"\"You are an expert evaluator of questions about {format_subject(subject)}.\n",
        "Determine if the proposed answer is correct. Output ONLY 'A' or 'B'.\n",
        "Question: {question}\n",
        "Proposed Answer: {proposed_answer}\n",
        "\n",
        "Is this answer correct? Respond ONLY with:\n",
        "A. Correct\n",
        "B. Incorrect\n",
        "\n",
        "Answer:\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def get_discriminator_probs(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_text,\n",
        "    choices_list\n",
        "):\n",
        "    input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    logits = model(input_ids=input_ids).logits[0, -1]\n",
        "\n",
        "    choice_logits = torch.tensor(\n",
        "        [\n",
        "            logits[tokenizer(\"A\").input_ids[-1]],\n",
        "            logits[tokenizer(\"B\").input_ids[-1]],\n",
        "        ]\n",
        "    ).float()\n",
        "\n",
        "    disc_dict = {\"A\":\"correct\", \"B\":\"incorrect\"}\n",
        "    probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
        "\n",
        "    choices = [f\"{chr(65+i)}\" for i, choice in enumerate(choices_list)]\n",
        "    choice_probs = {disc_dict[choice]: prob for choice, prob in zip(choices, probs)}\n",
        "\n",
        "    return choice_probs\n",
        "\n",
        "\n",
        "def evaluate_answer_correctness(\n",
        "    row,\n",
        "    model,\n",
        "    tokenizer\n",
        "):\n",
        "    subject = row[\"subject\"]\n",
        "    question = row[\"question\"]\n",
        "    choices = row[\"choices\"]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for idx, answer in enumerate(choices):\n",
        "        prompt = build_discriminator_prompt(\n",
        "            subject=subject,\n",
        "            question=question,\n",
        "            proposed_answer=f\"{answer}\"\n",
        "        )\n",
        "\n",
        "        probs = get_discriminator_probs(model, tokenizer, prompt, choices)\n",
        "\n",
        "        disc_dict_answer = {i: f\"{chr(65+i)}\" for i, choice in enumerate(row[\"choices\"])}\n",
        "\n",
        "        results[disc_dict_answer[idx]] = probs\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def get_initial_discriminator_probs(\n",
        "    row,\n",
        "    model,\n",
        "    tokenizer\n",
        "):\n",
        "    disc_init = evaluate_answer_correctness(row, model, tokenizer)\n",
        "    return disc_init\n",
        "\n",
        "\n",
        "def pick_answer(gen, disc, candidates, method=\"generator\"):\n",
        "    if method == \"generator\":\n",
        "        best_y = None\n",
        "        best_prob = -1.0\n",
        "        for y in candidates:\n",
        "            p = gen[\"correct\"][y]\n",
        "            if p > best_prob:\n",
        "                best_prob = p\n",
        "                best_y = y\n",
        "        return best_y\n",
        "    else:\n",
        "        best_y = None\n",
        "        best_prob = -1.0\n",
        "        for y in candidates:\n",
        "            p = disc[y][\"correct\"]\n",
        "            if p > best_prob:\n",
        "                best_prob = p\n",
        "                best_y = y\n",
        "        return best_y\n",
        "\n",
        "\n",
        "def softmax(arr):\n",
        "    m = np.max(arr)\n",
        "    exp_vals = np.exp(arr - m)\n",
        "    return exp_vals / np.sum(exp_vals)\n",
        "\n",
        "\n",
        "def equilibrium_search(gen_init, disc_init,\n",
        "                       candidates,\n",
        "                       T=5000,\n",
        "                       eta_G=0.1, eta_D=0.1,\n",
        "                       lam_G=0.1, lam_D=0.01):\n",
        "    gen = {\"correct\": dict(gen_init[\"correct\"]),\n",
        "           \"incorrect\": dict(gen_init[\"incorrect\"])}\n",
        "    disc = {}\n",
        "    for y in candidates:\n",
        "        disc[y] = dict(disc_init[y])\n",
        "\n",
        "    Qg = {\"correct\": {y: 0.0 for y in candidates},\n",
        "          \"incorrect\": {y: 0.0 for y in candidates}}\n",
        "    Qd = {y: {\"correct\": 0.0, \"incorrect\": 0.0} for y in candidates}\n",
        "\n",
        "    for t in range(1, T+1):\n",
        "        for v in [\"correct\", \"incorrect\"]:\n",
        "            for y in candidates:\n",
        "                Qg[v][y] += (1.0/(2.0*t)) * disc[y][v]\n",
        "\n",
        "        for y in candidates:\n",
        "            for v in [\"correct\", \"incorrect\"]:\n",
        "                Qd[y][v] += (1.0/(2.0*t)) * gen[v][y]\n",
        "\n",
        "        for v in [\"correct\", \"incorrect\"]:\n",
        "            logits = []\n",
        "            for y in candidates:\n",
        "                val = (Qg[v][y] + lam_G * math.log(gen_init[v][y] + 1e-12) )/ (1/eta_G  + lam_G)\n",
        "                logits.append(val)\n",
        "\n",
        "            new_probs = softmax(np.array(logits))\n",
        "\n",
        "            for i, y in enumerate(candidates):\n",
        "                gen[v][y] = new_probs[i]\n",
        "\n",
        "        logits_correct = []\n",
        "        logits_incorrect = []\n",
        "        for y in candidates:\n",
        "            val_correct = (Qd[y][\"correct\"] + lam_D * math.log(disc_init[y][\"correct\"] + 1e-12)) / (1/eta_D + lam_D)\n",
        "            logits_correct.append(val_correct)\n",
        "\n",
        "            val_incorrect = (Qd[y][\"incorrect\"] + lam_D * math.log(disc_init[y][\"incorrect\"] + 1e-12)) / (1/eta_D + lam_D)\n",
        "            logits_incorrect.append(val_incorrect)\n",
        "\n",
        "        new_probs_correct = softmax(np.array(logits_correct))\n",
        "        new_probs_incorrect = softmax(np.array(logits_incorrect))\n",
        "\n",
        "        for i, y in enumerate(candidates):\n",
        "            disc[y][\"correct\"] = new_probs_correct[i]\n",
        "            disc[y][\"incorrect\"] = new_probs_incorrect[i]\n",
        "\n",
        "    return gen, disc\n",
        "\n",
        "\n",
        "def load_model(model_name):\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        load_in_8bit=False,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"cuda\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def subcategory_df_function(model_gen, tokenizer_gen, model_disc, tokenizer_disc, df):\n",
        "    category_df = df.copy()\n",
        "\n",
        "    gen_answer = []\n",
        "    disc_answer = []\n",
        "    gen_init_answer = []\n",
        "    disc_init_answer = []\n",
        "    disc_init_policy = []\n",
        "    gen_init_policy = []\n",
        "\n",
        "    disc_final_policy_consensus = []\n",
        "    gen_final_policy_consensus = []\n",
        "\n",
        "    for _, row in tqdm(category_df.iterrows(), total=len(category_df)):\n",
        "        disc_init = get_initial_discriminator_probs(row, model_disc, tokenizer_disc)\n",
        "        disc_init_policy.append(disc_init)\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        gen_init = get_initial_generator_probs(row, model_gen, tokenizer_gen)\n",
        "        gen_init_policy.append(gen_init)\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        gen_init_answer.append(max(gen_init[\"correct\"], key=gen_init[\"correct\"].get))\n",
        "        disc_init_answer.append(max(disc_init, key=lambda choice: disc_init[choice][\"correct\"]))\n",
        "\n",
        "        candidates = [f\"{chr(65+i)}\" for i, choice in enumerate(row[\"choices\"])]\n",
        "\n",
        "        gen_final, disc_final = equilibrium_search(\n",
        "            gen_init, disc_init, candidates,\n",
        "            T=20, eta_G=0.1, eta_D=0.1, lam_G=0.1, lam_D=0.1\n",
        "        )\n",
        "        disc_final_policy_consensus.append(disc_final)\n",
        "        gen_final_policy_consensus.append(gen_final)\n",
        "\n",
        "        best_answer_g = pick_answer(gen_final, disc_final, candidates, method=\"generator\")\n",
        "        best_answer_d = pick_answer(gen_final, disc_final, candidates, method=\"discriminator\")\n",
        "\n",
        "        gen_answer.append(best_answer_g)\n",
        "        disc_answer.append(best_answer_d)\n",
        "\n",
        "    category_df[\"gen_init_answer\"] = gen_init_answer\n",
        "    category_df[\"disc_answer\"] = disc_answer\n",
        "    category_df[\"gen_answer\"] = gen_answer\n",
        "    category_df[\"disc_init_answer\"] = disc_init_answer\n",
        "    category_df[\"disc_final_policy_consensus\"] = disc_final_policy_consensus\n",
        "    category_df[\"disc_init_policy\"] = disc_init_policy\n",
        "    category_df[\"gen_init_policy\"] = gen_init_policy\n",
        "    category_df[\"gen_final_policy_consensus\"] = gen_final_policy_consensus\n",
        "\n",
        "    return category_df\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "if 'model_gen' in globals():\n",
        "    del model_gen\n",
        "if 'tokenizer_gen' in globals():\n",
        "    del tokenizer_gen\n",
        "if 'model_disc' in globals():\n",
        "    del model_disc\n",
        "if 'tokenizer_disc' in globals():\n",
        "    del tokenizer_disc\n",
        "\n",
        "print(\"Loading generator model: microsoft/phi-2\")\n",
        "model_gen, tokenizer_gen = load_model(\"microsoft/phi-2\")\n",
        "\n",
        "print(\"Loading discriminator model: Qwen/Qwen2-1.5B-Instruct\")\n",
        "model_disc, tokenizer_disc = load_model(\"Qwen/Qwen2-1.5B-Instruct\")\n",
        "\n",
        "print(\"Processing ARC-Challenge dataset...\")\n",
        "temp_df_phi2_qwen = subcategory_df_function(model_gen, tokenizer_gen, model_disc, tokenizer_disc, arc_df)\n",
        "\n",
        "file_path = 'Data/arc_policy_df_phi2_gen_qwen_disc.csv'\n",
        "temp_df_phi2_qwen.to_csv(file_path, index=False)\n",
        "print(f\"Saved results to {file_path}\")\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "del model_gen\n",
        "del tokenizer_gen\n",
        "del model_disc\n",
        "del tokenizer_disc\n",
        "\n",
        "print(\"\\nLoading ARC-Easy dataset...\")\n",
        "arc_data_easy = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\", split=\"test\")\n",
        "arc_df_easy = arc_data_easy.to_pandas()\n",
        "arc_df_easy = arc_df_easy.drop_duplicates(subset=['question'])\n",
        "arc_df_easy[\"choices_dic\"] = arc_df_easy[\"choices\"]\n",
        "arc_df_easy[\"choices\"] = arc_df_easy[\"choices\"].apply(lambda x: x[\"text\"])\n",
        "arc_df_easy[\"subject\"] = \"science\"\n",
        "print(f\"ARC-Easy shape: {arc_df_easy.shape}\")\n",
        "\n",
        "print(\"Reloading models for ARC-Easy...\")\n",
        "model_gen, tokenizer_gen = load_model(\"microsoft/phi-2\")\n",
        "model_disc, tokenizer_disc = load_model(\"Qwen/Qwen2-1.5B-Instruct\")\n",
        "\n",
        "print(\"Processing ARC-Easy dataset...\")\n",
        "temp_df_phi2_qwen_easy = subcategory_df_function(model_gen, tokenizer_gen, model_disc, tokenizer_disc, arc_df_easy)\n",
        "\n",
        "file_path_easy = 'Data/arc_policy_df_easy_phi2_gen_qwen_disc.csv'\n",
        "temp_df_phi2_qwen_easy.to_csv(file_path_easy, index=False)\n",
        "print(f\"Saved results to {file_path_easy}\")\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ]
    }
  ]
}
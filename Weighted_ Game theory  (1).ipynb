{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdfffad-6062-4835-b04f-d9b50f68b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PerformanceConfig:\n",
    "    \"\"\"Centralized configuration for performance optimizations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device_name = torch.cuda.get_device_name() if torch.cuda.is_available() else \"CPU\"\n",
    "        self.use_fp16 = self.device == \"cuda\"\n",
    "        self.dtype = torch.float16 if self.use_fp16 else torch.float32\n",
    "        self.clear_cache_every = 50\n",
    "        self.equilibrium_iterations = 5\n",
    "        self.early_stop_threshold = 0.001\n",
    "        \n",
    "        print(f\"Performance Configuration:\")\n",
    "        print(f\"  Device: {self.device} ({self.device_name})\")\n",
    "        print(f\"  Precision: {'FP16' if self.use_fp16 else 'FP32'}\")\n",
    "\n",
    "PERF_CONFIG = PerformanceConfig()\n",
    "\n",
    "def load_arc_datasets():\n",
    "    \"\"\"Load both ARC Challenge and Easy datasets.\"\"\"\n",
    "    print(\"Loading ARC Challenge dataset...\")\n",
    "    arc_data = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
    "    arc_df = arc_data.to_pandas()\n",
    "    arc_df = arc_df.drop_duplicates(subset=['question'])\n",
    "    arc_df[\"choices\"] = arc_df[\"choices\"].apply(lambda x: x[\"text\"])\n",
    "    arc_df[\"subject\"] = \"science\"\n",
    "    print(f\"ARC Challenge shape: {arc_df.shape}\")\n",
    "    \n",
    "    print(\"Loading ARC Easy dataset...\")\n",
    "    arc_easy_data = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\", split=\"test\")\n",
    "    arc_easy_df = arc_easy_data.to_pandas()\n",
    "    arc_easy_df = arc_easy_df.drop_duplicates(subset=['question'])\n",
    "    arc_easy_df[\"choices\"] = arc_easy_df[\"choices\"].apply(lambda x: x[\"text\"])\n",
    "    arc_easy_df[\"subject\"] = \"science\"\n",
    "    print(f\"ARC Easy shape: {arc_easy_df.shape}\")\n",
    "    \n",
    "    return arc_df, arc_easy_df\n",
    "\n",
    "#==============================================================================\n",
    "# HYBRID CALIBRATION + GAME SOFTMAX CLASSES\n",
    "#==============================================================================\n",
    "\n",
    "class HybridCalibrationGameSoftmax:\n",
    "    \"\"\"\n",
    "    Core hybrid approach: Calibrated weights for foundation + Softmax for game dynamics\n",
    "    Phase 1: Establish base model strengths through comprehensive calibration\n",
    "    Phase 2: Apply those strengths dynamically in game theory with softmax\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, preprocessing_system, adversarial_framework):\n",
    "        self.preprocessing = preprocessing_system\n",
    "        self.adversarial = adversarial_framework\n",
    "        \n",
    "        # Phase 1: Base strengths from calibration\n",
    "        self.base_phi2_strength = 0.5\n",
    "        self.base_qwen2_strength = 0.5\n",
    "        self.calibration_confidence = 0.5\n",
    "        self.calibration_details = {}\n",
    "        \n",
    "        # Phase 2: Game dynamics tracking\n",
    "        self.temperature_history = []\n",
    "        self.weight_evolution = []\n",
    "        \n",
    "    def calibrate_base_strengths(self, calibration_df, num_questions=50, method='comprehensive'):\n",
    "        \"\"\"\n",
    "        Phase 1: Establish empirically-grounded base model strengths\n",
    "        This gives us the foundation logits for game theory\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"PHASE 1: CALIBRATING BASE MODEL STRENGTHS\")\n",
    "        print(f\"Method: {method.upper()}\")\n",
    "        print(f\"Questions: {num_questions}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        sample_df = calibration_df.sample(n=min(num_questions, len(calibration_df)), random_state=42)\n",
    "        \n",
    "        if method == 'comprehensive':\n",
    "            strengths = self._comprehensive_strength_calibration(sample_df)\n",
    "        elif method == 'adversarial_aware':\n",
    "            strengths = self._adversarial_aware_calibration(sample_df)\n",
    "        else:\n",
    "            strengths = self._simple_strength_calibration(sample_df)\n",
    "        \n",
    "        # Store base strengths (these become our logits for game theory)\n",
    "        self.base_phi2_strength = strengths['phi2_strength']\n",
    "        self.base_qwen2_strength = strengths['qwen2_strength']\n",
    "        self.calibration_confidence = strengths['confidence']\n",
    "        self.calibration_details = strengths\n",
    "        \n",
    "        print(f\"\\nBase Model Strengths Established:\")\n",
    "        print(f\"  Phi-2 base strength: {self.base_phi2_strength:.3f}\")\n",
    "        print(f\"  Qwen2 base strength: {self.base_qwen2_strength:.3f}\")\n",
    "        print(f\"  Calibration confidence: {self.calibration_confidence:.3f}\")\n",
    "        print(f\"  Method used: {method}\")\n",
    "        \n",
    "        return strengths\n",
    "    \n",
    "    def _comprehensive_strength_calibration(self, sample_df):\n",
    "        \"\"\"Comprehensive calibration considering multiple factors\"\"\"\n",
    "        \n",
    "        phi2_metrics = {'accuracy': 0, 'confidence': 0, 'consistency': 0, 'robustness': 0}\n",
    "        qwen2_metrics = {'accuracy': 0, 'confidence': 0, 'consistency': 0, 'robustness': 0}\n",
    "        \n",
    "        phi2_predictions = []\n",
    "        qwen2_predictions = []\n",
    "        correct_answers = []\n",
    "        phi2_confidences = []\n",
    "        qwen2_confidences = []\n",
    "        \n",
    "        print(\"  Analyzing model performance across multiple criteria...\")\n",
    "        \n",
    "        for idx, (_, row) in enumerate(sample_df.iterrows()):\n",
    "            question = row[\"question\"]\n",
    "            choices = row[\"choices\"]\n",
    "            correct_answer = row.get(\"answerKey\")\n",
    "            \n",
    "            if not correct_answer:\n",
    "                continue\n",
    "                \n",
    "            correct_idx = ord(correct_answer) - ord('A')\n",
    "            prompt = self.preprocessing.build_analysis_prompt(question, choices)\n",
    "            \n",
    "            # Get model responses\n",
    "            phi2_probs = self.preprocessing.get_model_confidence_scores(\n",
    "                self.preprocessing.phi2_model, self.preprocessing.phi2_tokenizer, prompt, len(choices)\n",
    "            )\n",
    "            qwen2_probs = self.preprocessing.get_model_confidence_scores(\n",
    "                self.preprocessing.qwen2_model, self.preprocessing.qwen2_tokenizer, prompt, len(choices)\n",
    "            )\n",
    "            \n",
    "            phi2_pred = np.argmax(phi2_probs)\n",
    "            qwen2_pred = np.argmax(qwen2_probs)\n",
    "            phi2_conf = phi2_probs[phi2_pred]\n",
    "            qwen2_conf = qwen2_probs[qwen2_pred]\n",
    "            \n",
    "            # Store for analysis\n",
    "            phi2_predictions.append(phi2_pred)\n",
    "            qwen2_predictions.append(qwen2_pred)\n",
    "            correct_answers.append(correct_idx)\n",
    "            phi2_confidences.append(phi2_conf)\n",
    "            qwen2_confidences.append(qwen2_conf)\n",
    "            \n",
    "            # Update metrics\n",
    "            phi2_metrics['accuracy'] += (phi2_pred == correct_idx)\n",
    "            qwen2_metrics['accuracy'] += (qwen2_pred == correct_idx)\n",
    "            phi2_metrics['confidence'] += phi2_conf\n",
    "            qwen2_metrics['confidence'] += qwen2_conf\n",
    "            \n",
    "            # Progress update\n",
    "            if (idx + 1) % 10 == 0 or idx == len(sample_df) - 1:\n",
    "                print(f\"    Processed {idx + 1}/{len(sample_df)} questions...\")\n",
    "        \n",
    "        n = len(sample_df)\n",
    "        \n",
    "        # Normalize basic metrics\n",
    "        phi2_metrics['accuracy'] /= n\n",
    "        qwen2_metrics['accuracy'] /= n\n",
    "        phi2_metrics['confidence'] /= n\n",
    "        qwen2_metrics['confidence'] /= n\n",
    "        \n",
    "        # Calculate advanced metrics\n",
    "        phi2_metrics['consistency'] = self._calculate_consistency(phi2_predictions)\n",
    "        qwen2_metrics['consistency'] = self._calculate_consistency(qwen2_predictions)\n",
    "        \n",
    "        phi2_metrics['robustness'] = self._calculate_robustness(phi2_confidences, phi2_predictions, correct_answers)\n",
    "        qwen2_metrics['robustness'] = self._calculate_robustness(qwen2_confidences, qwen2_predictions, correct_answers)\n",
    "        \n",
    "        # Calculate complementarity\n",
    "        complementarity = self._calculate_complementarity(phi2_predictions, qwen2_predictions, correct_answers)\n",
    "        \n",
    "        # Combine into strength scores with weighted criteria\n",
    "        criteria_weights = {\n",
    "            'accuracy': 0.4,      # Most important: raw performance\n",
    "            'confidence': 0.2,    # How confident when making predictions\n",
    "            'consistency': 0.2,   # How consistent across questions\n",
    "            'robustness': 0.2     # High confidence when correct\n",
    "        }\n",
    "        \n",
    "        phi2_strength = sum(criteria_weights[k] * phi2_metrics[k] for k in criteria_weights)\n",
    "        qwen2_strength = sum(criteria_weights[k] * qwen2_metrics[k] for k in criteria_weights)\n",
    "        \n",
    "        # Calculate calibration confidence (how reliable these strengths are)\n",
    "        strength_variance = abs(phi2_strength - qwen2_strength)\n",
    "        confidence = min(1.0, 0.3 + 0.7 * (1 - strength_variance))  # Higher difference = higher confidence\n",
    "        \n",
    "        print(f\"  Comprehensive Analysis Results:\")\n",
    "        print(f\"    Phi-2 - Accuracy: {phi2_metrics['accuracy']:.1%}, Confidence: {phi2_metrics['confidence']:.3f}\")\n",
    "        print(f\"           Consistency: {phi2_metrics['consistency']:.3f}, Robustness: {phi2_metrics['robustness']:.3f}\")\n",
    "        print(f\"    Qwen2 - Accuracy: {qwen2_metrics['accuracy']:.1%}, Confidence: {qwen2_metrics['confidence']:.3f}\")\n",
    "        print(f\"           Consistency: {qwen2_metrics['consistency']:.3f}, Robustness: {qwen2_metrics['robustness']:.3f}\")\n",
    "        print(f\"    Complementarity: {complementarity:.1%}\")\n",
    "        print(f\"    Combined strengths: Phi-2={phi2_strength:.3f}, Qwen2={qwen2_strength:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'phi2_strength': phi2_strength,\n",
    "            'qwen2_strength': qwen2_strength,\n",
    "            'confidence': confidence,\n",
    "            'phi2_metrics': phi2_metrics,\n",
    "            'qwen2_metrics': qwen2_metrics,\n",
    "            'complementarity': complementarity,\n",
    "            'sample_size': n,\n",
    "            'method': 'comprehensive'\n",
    "        }\n",
    "    \n",
    "    def _simple_strength_calibration(self, sample_df):\n",
    "        \"\"\"Simple accuracy-based strength calibration\"\"\"\n",
    "        phi2_correct = 0\n",
    "        qwen2_correct = 0\n",
    "        total_phi2_conf = 0\n",
    "        total_qwen2_conf = 0\n",
    "        \n",
    "        print(\"  Running simple accuracy-based calibration...\")\n",
    "        \n",
    "        for idx, (_, row) in enumerate(sample_df.iterrows()):\n",
    "            question = row[\"question\"]\n",
    "            choices = row[\"choices\"]\n",
    "            correct_answer = row.get(\"answerKey\")\n",
    "            \n",
    "            if not correct_answer:\n",
    "                continue\n",
    "                \n",
    "            correct_idx = ord(correct_answer) - ord('A')\n",
    "            prompt = self.preprocessing.build_analysis_prompt(question, choices)\n",
    "            \n",
    "            phi2_probs = self.preprocessing.get_model_confidence_scores(\n",
    "                self.preprocessing.phi2_model, self.preprocessing.phi2_tokenizer, prompt, len(choices)\n",
    "            )\n",
    "            qwen2_probs = self.preprocessing.get_model_confidence_scores(\n",
    "                self.preprocessing.qwen2_model, self.preprocessing.qwen2_tokenizer, prompt, len(choices)\n",
    "            )\n",
    "            \n",
    "            phi2_pred = np.argmax(phi2_probs)\n",
    "            qwen2_pred = np.argmax(qwen2_probs)\n",
    "            \n",
    "            if phi2_pred == correct_idx:\n",
    "                phi2_correct += 1\n",
    "            if qwen2_pred == correct_idx:\n",
    "                qwen2_correct += 1\n",
    "                \n",
    "            total_phi2_conf += phi2_probs[phi2_pred]\n",
    "            total_qwen2_conf += qwen2_probs[qwen2_pred]\n",
    "            \n",
    "            # Progress update\n",
    "            if (idx + 1) % 10 == 0 or idx == len(sample_df) - 1:\n",
    "                print(f\"    Processed {idx + 1}/{len(sample_df)} questions...\")\n",
    "        \n",
    "        phi2_strength = phi2_correct / len(sample_df)\n",
    "        qwen2_strength = qwen2_correct / len(sample_df)\n",
    "        avg_phi2_conf = total_phi2_conf / len(sample_df)\n",
    "        avg_qwen2_conf = total_qwen2_conf / len(sample_df)\n",
    "        \n",
    "        # Confidence based on how clear the winner is\n",
    "        strength_diff = abs(phi2_strength - qwen2_strength)\n",
    "        confidence = min(1.0, 0.5 + strength_diff)\n",
    "        \n",
    "        print(f\"  Simple Calibration Results:\")\n",
    "        print(f\"    Phi-2: {phi2_strength:.1%} accuracy, {avg_phi2_conf:.3f} avg confidence\")\n",
    "        print(f\"    Qwen2: {qwen2_strength:.1%} accuracy, {avg_qwen2_conf:.3f} avg confidence\")\n",
    "        \n",
    "        return {\n",
    "            'phi2_strength': phi2_strength,\n",
    "            'qwen2_strength': qwen2_strength,\n",
    "            'confidence': confidence,\n",
    "            'phi2_accuracy': phi2_strength,\n",
    "            'qwen2_accuracy': qwen2_strength,\n",
    "            'phi2_avg_confidence': avg_phi2_conf,\n",
    "            'qwen2_avg_confidence': avg_qwen2_conf,\n",
    "            'sample_size': len(sample_df),\n",
    "            'method': 'simple'\n",
    "        }\n",
    "    \n",
    "    def _adversarial_aware_calibration(self, sample_df):\n",
    "        \"\"\"Calibration that considers adversarial dynamics\"\"\"\n",
    "        # This would run mini adversarial games during calibration\n",
    "        # For now, fall back to comprehensive method\n",
    "        print(\"  Running adversarial-aware calibration (using comprehensive method)...\")\n",
    "        return self._comprehensive_strength_calibration(sample_df)\n",
    "    \n",
    "    def game_theory_softmax_equilibrium(self, gen_init, disc_init, candidates, T=None,\n",
    "                                       temperature_strategy='adaptive'):\n",
    "        \"\"\"\n",
    "        Phase 2: Use calibrated base strengths in game theory with softmax dynamics\n",
    "        \"\"\"\n",
    "        if T is None:\n",
    "            T = PERF_CONFIG.equilibrium_iterations\n",
    "            \n",
    "        # Initialize with base strengths\n",
    "        gen = {\"correct\": dict(gen_init[\"correct\"]), \"incorrect\": dict(gen_init[\"incorrect\"])}\n",
    "        disc = {y: dict(disc_init[y]) for y in candidates}\n",
    "        \n",
    "        # Q-values for learning\n",
    "        Qg = {\"correct\": {y: 0.0 for y in candidates}, \"incorrect\": {y: 0.0 for y in candidates}}\n",
    "        Qd = {y: {\"correct\": 0.0, \"incorrect\": 0.0} for y in candidates}\n",
    "        \n",
    "        equilibrium_history = []\n",
    "        \n",
    "        for t in range(1, T+1):\n",
    "            # Determine temperature for this iteration\n",
    "            temperature = self._get_temperature(t, T, temperature_strategy, gen, disc, candidates)\n",
    "            \n",
    "            # Apply base strengths with current temperature to get dynamic weights\n",
    "            base_logits = torch.tensor([self.base_phi2_strength, self.base_qwen2_strength], dtype=torch.float32)\n",
    "            current_weights = F.softmax(base_logits / temperature, dim=0)\n",
    "            \n",
    "            gen_weight = float(current_weights[0])  # Phi-2 weight (generator)\n",
    "            disc_weight = float(current_weights[1])  # Qwen2 weight (discriminator)\n",
    "            \n",
    "            # Store iteration info\n",
    "            iteration_info = {\n",
    "                'iteration': t,\n",
    "                'temperature': temperature,\n",
    "                'gen_weight': gen_weight,\n",
    "                'disc_weight': disc_weight,\n",
    "                'base_phi2_strength': self.base_phi2_strength,\n",
    "                'base_qwen2_strength': self.base_qwen2_strength\n",
    "            }\n",
    "            \n",
    "            # Update Q-values with current weights\n",
    "            for v in [\"correct\", \"incorrect\"]:\n",
    "                for y in candidates:\n",
    "                    # Discriminator feedback weighted by its current strength\n",
    "                    Qg[v][y] += (disc_weight / (2.0 * t)) * disc[y][v]\n",
    "            \n",
    "            for y in candidates:\n",
    "                for v in [\"correct\", \"incorrect\"]:\n",
    "                    # Generator feedback weighted by its current strength\n",
    "                    Qd[y][v] += (gen_weight / (2.0 * t)) * gen[v][y]\n",
    "            \n",
    "            # Update generator policy (with weighted learning rate)\n",
    "            eta_G = 0.1 * gen_weight\n",
    "            for v in [\"correct\", \"incorrect\"]:\n",
    "                logits = []\n",
    "                for y in candidates:\n",
    "                    val = (Qg[v][y] + 0.1 * math.log(gen_init[v][y] + 1e-12)) / (1/eta_G + 0.1)\n",
    "                    logits.append(val)\n",
    "                \n",
    "                if max(logits) > min(logits):\n",
    "                    new_probs = self._softmax(np.array(logits))\n",
    "                    for i, y in enumerate(candidates):\n",
    "                        gen[v][y] = new_probs[i]\n",
    "            \n",
    "            # Update discriminator policy (with weighted learning rate)\n",
    "            eta_D = 0.1 * disc_weight\n",
    "            for y in candidates:\n",
    "                logits = [\n",
    "                    (Qd[y][\"correct\"] + 0.01 * math.log(disc_init[y][\"correct\"] + 1e-12)) / (1/eta_D + 0.01),\n",
    "                    (Qd[y][\"incorrect\"] + 0.01 * math.log(disc_init[y][\"incorrect\"] + 1e-12)) / (1/eta_D + 0.01)\n",
    "                ]\n",
    "                \n",
    "                if max(logits) > min(logits):\n",
    "                    probs = self._softmax(np.array(logits))\n",
    "                    disc[y][\"correct\"] = probs[0]\n",
    "                    disc[y][\"incorrect\"] = probs[1]\n",
    "            \n",
    "            # Evaluate convergence\n",
    "            convergence_metrics = self._evaluate_convergence(gen, disc, candidates, t, T)\n",
    "            iteration_info.update(convergence_metrics)\n",
    "            \n",
    "            equilibrium_history.append(iteration_info)\n",
    "            \n",
    "            # Early stopping if converged\n",
    "            if convergence_metrics.get('converged', False):\n",
    "                break\n",
    "        \n",
    "        # Store history for analysis\n",
    "        self.temperature_history.extend([info['temperature'] for info in equilibrium_history])\n",
    "        self.weight_evolution.extend(equilibrium_history)\n",
    "        \n",
    "        return {\n",
    "            'gen_final': gen,\n",
    "            'disc_final': disc,\n",
    "            'equilibrium_history': equilibrium_history,\n",
    "            'final_weights': {\n",
    "                'gen_weight': equilibrium_history[-1]['gen_weight'],\n",
    "                'disc_weight': equilibrium_history[-1]['disc_weight']\n",
    "            },\n",
    "            'convergence_iteration': len(equilibrium_history),\n",
    "            'temperature_strategy': temperature_strategy\n",
    "        }\n",
    "    \n",
    "    def _get_temperature(self, iteration, total_iterations, strategy, gen, disc, candidates):\n",
    "        \"\"\"Determine temperature for current iteration\"\"\"\n",
    "        \n",
    "        if strategy == 'fixed':\n",
    "            return 1.0\n",
    "        \n",
    "        elif strategy == 'annealing':\n",
    "            # Start high, cool down linearly\n",
    "            return 2.0 * (total_iterations - iteration) / total_iterations + 0.5\n",
    "        \n",
    "        elif strategy == 'adaptive':\n",
    "            # Adapt based on calibration confidence and game state\n",
    "            \n",
    "            # Base temperature from calibration confidence\n",
    "            base_temp = 0.5 + (2.0 - 0.5) * (1 - self.calibration_confidence)\n",
    "            \n",
    "            # Adjust based on iteration progress\n",
    "            if iteration <= 2:\n",
    "                # Early iterations: explore more\n",
    "                return base_temp * 1.5\n",
    "            \n",
    "            # Check strategy stability for later iterations\n",
    "            if iteration > 3:\n",
    "                stability = self._calculate_strategy_stability(gen, disc, candidates)\n",
    "                if stability > 0.95:  # Very stable\n",
    "                    return base_temp * 0.7  # Exploit more\n",
    "                elif stability < 0.8:  # Unstable\n",
    "                    return base_temp * 1.3  # Explore more\n",
    "            \n",
    "            return base_temp\n",
    "        \n",
    "        elif strategy == 'confidence_based':\n",
    "            # Temperature based purely on calibration confidence\n",
    "            return 0.5 + 1.5 * (1 - self.calibration_confidence)\n",
    "        \n",
    "        else:\n",
    "            return 1.0  # Default\n",
    "    \n",
    "    def _calculate_strategy_stability(self, gen, disc, candidates):\n",
    "        \"\"\"Calculate how stable the current strategies are\"\"\"\n",
    "        \n",
    "        # Generator stability (how concentrated is the \"correct\" strategy)\n",
    "        gen_probs = [gen[\"correct\"][y] for y in candidates]\n",
    "        gen_entropy = self._entropy(gen_probs)\n",
    "        \n",
    "        # Discriminator stability (how concentrated are the \"correct\" judgments)\n",
    "        disc_probs = [disc[y][\"correct\"] for y in candidates]\n",
    "        disc_entropy = self._entropy(disc_probs)\n",
    "        \n",
    "        # Normalize by maximum possible entropy\n",
    "        max_entropy = np.log(len(candidates))\n",
    "        if max_entropy > 0:\n",
    "            gen_stability = 1.0 - (gen_entropy / max_entropy)\n",
    "            disc_stability = 1.0 - (disc_entropy / max_entropy)\n",
    "        else:\n",
    "            gen_stability = disc_stability = 1.0\n",
    "        \n",
    "        # Overall stability\n",
    "        return (gen_stability + disc_stability) / 2\n",
    "    \n",
    "    def _evaluate_convergence(self, gen, disc, candidates, iteration, total_iterations):\n",
    "        \"\"\"Evaluate if equilibrium has converged\"\"\"\n",
    "        \n",
    "        # Check strategy concentration\n",
    "        gen_concentration = max(gen[\"correct\"][y] for y in candidates)\n",
    "        disc_concentration = max(disc[y][\"correct\"] for y in candidates)\n",
    "        \n",
    "        # Check early stopping criteria\n",
    "        converged = False\n",
    "        if iteration >= 3:  # Don't stop too early\n",
    "            if gen_concentration > 0.8 and disc_concentration > 0.8:\n",
    "                converged = True\n",
    "            elif iteration >= total_iterations:\n",
    "                converged = True\n",
    "        \n",
    "        return {\n",
    "            'converged': converged,\n",
    "            'gen_concentration': gen_concentration,\n",
    "            'disc_concentration': disc_concentration,\n",
    "            'stability': self._calculate_strategy_stability(gen, disc, candidates)\n",
    "        }\n",
    "    \n",
    "    def get_final_answer_with_hybrid_weights(self, equilibrium_result, candidates, correct_answer=None):\n",
    "        \"\"\"Get final answer using the hybrid approach results\"\"\"\n",
    "        \n",
    "        gen_final = equilibrium_result['gen_final']\n",
    "        disc_final = equilibrium_result['disc_final']\n",
    "        final_weights = equilibrium_result['final_weights']\n",
    "        \n",
    "        # Use the final dynamic weights from the game\n",
    "        gen_weight = final_weights['gen_weight']\n",
    "        disc_weight = final_weights['disc_weight']\n",
    "        \n",
    "        # Get individual predictions\n",
    "        gen_answer = max(candidates, key=lambda x: gen_final[\"correct\"][x])\n",
    "        disc_answer = max(candidates, key=lambda x: disc_final[x][\"correct\"])\n",
    "        \n",
    "        # Weighted combination for final answer\n",
    "        combined_scores = {}\n",
    "        for y in candidates:\n",
    "            gen_score = gen_final[\"correct\"][y]\n",
    "            disc_score = disc_final[y][\"correct\"]\n",
    "            combined_scores[y] = gen_weight * gen_score + disc_weight * disc_score\n",
    "        \n",
    "        best_answer = max(candidates, key=lambda x: combined_scores[x])\n",
    "        \n",
    "        # Calculate accuracy if correct answer provided\n",
    "        accuracy = 0.0  # Default to 0.0 instead of None\n",
    "        if correct_answer:\n",
    "            accuracy = 1.0 if best_answer == correct_answer else 0.0\n",
    "        \n",
    "        return {\n",
    "            'final_answer': best_answer,\n",
    "            'gen_answer': gen_answer,\n",
    "            'disc_answer': disc_answer,\n",
    "            'combined_scores': combined_scores,\n",
    "            'weights_used': final_weights,\n",
    "            'base_strengths': {\n",
    "                'phi2': self.base_phi2_strength,\n",
    "                'qwen2': self.base_qwen2_strength\n",
    "            },\n",
    "            'weight_evolution': equilibrium_result['equilibrium_history'],\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "    \n",
    "    # Helper methods\n",
    "    def _calculate_consistency(self, predictions):\n",
    "        \"\"\"Calculate prediction consistency (1 - normalized entropy)\"\"\"\n",
    "        if not predictions:\n",
    "            return 0.0\n",
    "        unique, counts = np.unique(predictions, return_counts=True)\n",
    "        probs = counts / len(predictions)\n",
    "        entropy = -sum(p * np.log(p + 1e-12) for p in probs if p > 0)\n",
    "        max_entropy = np.log(len(unique)) if len(unique) > 1 else 1\n",
    "        return 1.0 - (entropy / max_entropy) if max_entropy > 0 else 1.0\n",
    "    \n",
    "    def _calculate_robustness(self, confidences, predictions, correct_answers):\n",
    "        \"\"\"Calculate prediction robustness (high confidence when correct)\"\"\"\n",
    "        correct_mask = np.array(predictions) == np.array(correct_answers)\n",
    "        if not any(correct_mask):\n",
    "            return 0.0\n",
    "        correct_confidences = np.array(confidences)[correct_mask]\n",
    "        return float(np.mean(correct_confidences))\n",
    "    \n",
    "    def _calculate_complementarity(self, phi2_preds, qwen2_preds, correct_answers):\n",
    "        \"\"\"Calculate how well models complement each other\"\"\"\n",
    "        phi2_correct = np.array(phi2_preds) == np.array(correct_answers)\n",
    "        qwen2_correct = np.array(qwen2_preds) == np.array(correct_answers)\n",
    "        \n",
    "        # Combined performance (either model correct)\n",
    "        either_correct = phi2_correct | qwen2_correct\n",
    "        combined_accuracy = np.mean(either_correct)\n",
    "        \n",
    "        # Best individual performance\n",
    "        best_individual = max(np.mean(phi2_correct), np.mean(qwen2_correct))\n",
    "        \n",
    "        # Complementarity = relative improvement from combination\n",
    "        if best_individual > 0:\n",
    "            return max(0.0, (combined_accuracy - best_individual) / best_individual)\n",
    "        else:\n",
    "            return 0.0\n",
    "    \n",
    "    def _softmax(self, arr):\n",
    "        \"\"\"Numerically stable softmax\"\"\"\n",
    "        m = np.max(arr)\n",
    "        exp_vals = np.exp(arr - m)\n",
    "        return exp_vals / np.sum(exp_vals)\n",
    "    \n",
    "    def _entropy(self, probs):\n",
    "        \"\"\"Calculate entropy of probability distribution\"\"\"\n",
    "        probs = np.array(probs)\n",
    "        probs = probs[probs > 0]  # Remove zeros\n",
    "        if len(probs) == 0:\n",
    "            return 0.0\n",
    "        return -np.sum(probs * np.log(probs + 1e-12))\n",
    "\n",
    "#==============================================================================\n",
    "# ENHANCED PREPROCESSING SYSTEM (Compatible with Hybrid)\n",
    "#==============================================================================\n",
    "\n",
    "class HybridOptimizedPreprocessingSystem:\n",
    "    \"\"\"\n",
    "    Enhanced preprocessing system that works with hybrid calibration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, confidence_threshold=0.10):\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.phi2_model = None\n",
    "        self.phi2_tokenizer = None\n",
    "        self.qwen2_model = None\n",
    "        self.qwen2_tokenizer = None\n",
    "        \n",
    "        # Weights (will be set by hybrid calibrator)\n",
    "        self.phi2_weight = 0.5\n",
    "        self.qwen2_weight = 0.5\n",
    "        self.weights_calibrated = False\n",
    "        self.calibration_results = None\n",
    "        \n",
    "    def load_preprocessing_models(self):\n",
    "        \"\"\"Load both models for preprocessing with optimizations.\"\"\"\n",
    "        print(f\"Loading preprocessing models on {PERF_CONFIG.device}...\")\n",
    "        \n",
    "        print(\"Loading Phi-2 for preprocessing...\")\n",
    "        self.phi2_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"microsoft/phi-2\",\n",
    "            torch_dtype=PERF_CONFIG.dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"auto\" if PERF_CONFIG.device == \"cuda\" else \"cpu\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.phi2_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"microsoft/phi-2\", \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        if self.phi2_tokenizer.pad_token is None:\n",
    "            self.phi2_tokenizer.pad_token = self.phi2_tokenizer.eos_token\n",
    "            \n",
    "        print(\"Loading Qwen2-1.5B for preprocessing...\")\n",
    "        self.qwen2_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "            torch_dtype=PERF_CONFIG.dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"auto\" if PERF_CONFIG.device == \"cuda\" else \"cpu\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.qwen2_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        if self.qwen2_tokenizer.pad_token is None:\n",
    "            self.qwen2_tokenizer.pad_token = self.qwen2_tokenizer.eos_token\n",
    "            \n",
    "        # Set models to eval mode\n",
    "        self.phi2_model.eval()\n",
    "        self.qwen2_model.eval()\n",
    "            \n",
    "        print(f\"Preprocessing models loaded successfully!\")\n",
    "        \n",
    "    def build_analysis_prompt(self, question, choices):\n",
    "        \"\"\"Build prompt for confidence analysis.\"\"\"\n",
    "        prompt = f\"\"\"The following is a multiple choice science question. Analyze all choices and select the most likely correct answer.\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "        for i, choice in enumerate(choices):\n",
    "            prompt += f\"{chr(65+i)}. {choice}\\n\"\n",
    "        prompt += \"\\nAnswer:\"\n",
    "        return prompt\n",
    "        \n",
    "    def get_model_confidence_scores(self, model, tokenizer, prompt, num_choices):\n",
    "        \"\"\"Get confidence scores from a model for answer choices.\"\"\"\n",
    "        try:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "            \n",
    "            # Move inputs to device if using GPU\n",
    "            if PERF_CONFIG.device == \"cuda\":\n",
    "                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast(enabled=PERF_CONFIG.use_fp16 and PERF_CONFIG.device == \"cuda\"):\n",
    "                    outputs = model(**inputs)\n",
    "                    logits = outputs.logits[0, -1]\n",
    "            \n",
    "            # Get probabilities for A, B, C, D based on number of choices\n",
    "            choice_logits = []\n",
    "            for i in range(num_choices):\n",
    "                letter = chr(65 + i)  # A, B, C, D\n",
    "                token_id = tokenizer.encode(letter, add_special_tokens=False)[-1]\n",
    "                choice_logits.append(logits[token_id].item())\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            choice_logits = torch.tensor(choice_logits)\n",
    "            probs = torch.nn.functional.softmax(choice_logits, dim=0).numpy()\n",
    "            \n",
    "            return probs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting confidence scores: {e}\")\n",
    "            # Return uniform distribution as fallback\n",
    "            return np.ones(num_choices) / num_choices\n",
    "    \n",
    "    def analyze_all_choices(self, question, choices):\n",
    "        \"\"\"Analyze all choices using both models with current weights\"\"\"\n",
    "        prompt = self.build_analysis_prompt(question, choices)\n",
    "        num_choices = len(choices)\n",
    "        \n",
    "        # Get confidence scores from both models\n",
    "        phi2_probs = self.get_model_confidence_scores(\n",
    "            self.phi2_model, self.phi2_tokenizer, prompt, num_choices\n",
    "        )\n",
    "        qwen2_probs = self.get_model_confidence_scores(\n",
    "            self.qwen2_model, self.qwen2_tokenizer, prompt, num_choices\n",
    "        )\n",
    "        \n",
    "        # Use current weights (set by hybrid calibrator)\n",
    "        combined_probs = self.phi2_weight * phi2_probs + self.qwen2_weight * qwen2_probs\n",
    "        \n",
    "        return {\n",
    "            'phi2_probs': phi2_probs,\n",
    "            'qwen2_probs': qwen2_probs,\n",
    "            'combined_probs': combined_probs,\n",
    "            'weights_used': {\n",
    "                'phi2': self.phi2_weight,\n",
    "                'qwen2': self.qwen2_weight\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def filter_low_confidence_choices(self, choices, combined_probs):\n",
    "        \"\"\"Filter out choices with prob < threshold\"\"\"\n",
    "        # Find choices above threshold\n",
    "        high_confidence_indices = [\n",
    "            i for i, prob in enumerate(combined_probs) \n",
    "            if prob >= self.confidence_threshold\n",
    "        ]\n",
    "        \n",
    "        # Fallback: keep all choices if none meet threshold\n",
    "        if not high_confidence_indices:\n",
    "            high_confidence_indices = list(range(len(choices)))\n",
    "        \n",
    "        # Extract high-confidence choices\n",
    "        filtered_choices = [choices[i] for i in high_confidence_indices]\n",
    "        filtered_probs = [combined_probs[i] for i in high_confidence_indices]\n",
    "        \n",
    "        return filtered_choices, filtered_probs, high_confidence_indices\n",
    "    \n",
    "    def randomize_choice_order(self, choices, probs, original_indices):\n",
    "        \"\"\"Randomly shuffle choices to prevent positional bias.\"\"\"\n",
    "        combined = list(zip(choices, probs, original_indices))\n",
    "        random.shuffle(combined)\n",
    "        \n",
    "        shuffled_choices, shuffled_probs, shuffled_original_indices = zip(*combined)\n",
    "        position_mapping = {i: orig_idx for i, orig_idx in enumerate(shuffled_original_indices)}\n",
    "        \n",
    "        return list(shuffled_choices), list(shuffled_probs), position_mapping\n",
    "    \n",
    "    def preprocess_question(self, question, choices, answer_key=None):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        # Analyze all choices\n",
    "        analysis_results = self.analyze_all_choices(question, choices)\n",
    "        \n",
    "        # Filter low-confidence choices\n",
    "        filtered_choices, filtered_probs, high_confidence_indices = self.filter_low_confidence_choices(\n",
    "            choices, analysis_results['combined_probs']\n",
    "        )\n",
    "        \n",
    "        # Randomize to prevent bias\n",
    "        final_choices, final_probs, position_mapping = self.randomize_choice_order(\n",
    "            filtered_choices, filtered_probs, high_confidence_indices\n",
    "        )\n",
    "        \n",
    "        # Update answer key if provided\n",
    "        new_answer_key = None\n",
    "        if answer_key:\n",
    "            original_answer_idx = ord(answer_key) - ord('A')\n",
    "            if original_answer_idx in high_confidence_indices:\n",
    "                for new_pos, orig_pos in position_mapping.items():\n",
    "                    if orig_pos == original_answer_idx:\n",
    "                        new_answer_key = chr(ord('A') + new_pos)\n",
    "                        break\n",
    "        \n",
    "        return {\n",
    "            'original_question': question,\n",
    "            'original_choices': choices,\n",
    "            'filtered_choices': final_choices,\n",
    "            'analysis_results': analysis_results,\n",
    "            'position_mapping': position_mapping,\n",
    "            'high_confidence_indices': high_confidence_indices,\n",
    "            'original_answer_key': answer_key,\n",
    "            'new_answer_key': new_answer_key,\n",
    "            'filtering_applied': len(final_choices) < len(choices)\n",
    "        }\n",
    "    \n",
    "    def cleanup_preprocessing_models(self):\n",
    "        \"\"\"Clean up preprocessing models.\"\"\"\n",
    "        if self.phi2_model:\n",
    "            del self.phi2_model\n",
    "        if self.phi2_tokenizer:\n",
    "            del self.phi2_tokenizer\n",
    "        if self.qwen2_model:\n",
    "            del self.qwen2_model\n",
    "        if self.qwen2_tokenizer:\n",
    "            del self.qwen2_tokenizer\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "#==============================================================================\n",
    "# ENHANCED ADVERSARIAL FRAMEWORK (Compatible with Hybrid)\n",
    "#==============================================================================\n",
    "\n",
    "class HybridOptimizedAdversarialFramework:\n",
    "    \"\"\"\n",
    "    Enhanced adversarial framework that works with hybrid calibration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.generator_model = None      # Phi-2\n",
    "        self.generator_tokenizer = None\n",
    "        self.discriminator_model = None  # Qwen2-1.5B\n",
    "        self.discriminator_tokenizer = None\n",
    "    \n",
    "    def setup_adversarial_models(self, phi2_model, phi2_tokenizer, qwen2_model, qwen2_tokenizer):\n",
    "        \"\"\"Set up adversarial roles\"\"\"\n",
    "        self.generator_model = phi2_model         # Phi-2 as Generator\n",
    "        self.generator_tokenizer = phi2_tokenizer\n",
    "        self.discriminator_model = qwen2_model    # Qwen2-1.5B as Discriminator\n",
    "        self.discriminator_tokenizer = qwen2_tokenizer\n",
    "        \n",
    "        print(\"Adversarial roles set:\")\n",
    "        print(\"  Generator: Phi-2\")\n",
    "        print(\"  Discriminator: Qwen2-1.5B-Instruct\")\n",
    "\n",
    "    def format_subject(self, subject):\n",
    "        \"\"\"Format subject string.\"\"\"\n",
    "        return \" \".join(subject.split(\"_\"))\n",
    "\n",
    "    def build_generator_prompt(self, subject, question, choices, get_correct):\n",
    "        \"\"\"Build prompt for generator.\"\"\"\n",
    "        prompt = f\"The following are multiple choice questions (with answers) about {self.format_subject(subject)}.\\n\\n\"\n",
    "        prompt += f\"{question}\"\n",
    "        \n",
    "        for i, choice in enumerate(choices):\n",
    "            prompt += f\"\\n{chr(65+i)}. {choice}\"\n",
    "            \n",
    "        if get_correct:\n",
    "            prompt += \"\\nAnswer:\"\n",
    "        else:\n",
    "            prompt += \"\\nIncorrect Answer:\"\n",
    "        return prompt\n",
    "\n",
    "    def get_generator_probabilities(self, prompt_text, num_choices):\n",
    "        \"\"\"Get generator answer probabilities.\"\"\"\n",
    "        try:\n",
    "            inputs = self.generator_tokenizer(prompt_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            \n",
    "            # Move to device if using GPU\n",
    "            if PERF_CONFIG.device == \"cuda\":\n",
    "                inputs = {k: v.to(self.generator_model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast(enabled=PERF_CONFIG.use_fp16 and PERF_CONFIG.device == \"cuda\"):\n",
    "                    outputs = self.generator_model(**inputs)\n",
    "                    logits = outputs.logits[0, -1]\n",
    "\n",
    "            # Get probabilities for A, B, C, D...\n",
    "            choice_logits = []\n",
    "            for i in range(num_choices):\n",
    "                letter = chr(65 + i)\n",
    "                token_id = self.generator_tokenizer.encode(letter, add_special_tokens=False)[-1]\n",
    "                choice_logits.append(logits[token_id].item())\n",
    "            \n",
    "            choice_logits = torch.tensor(choice_logits)\n",
    "            probs = torch.nn.functional.softmax(choice_logits, dim=0).numpy()\n",
    "            \n",
    "            return probs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in generator probabilities: {e}\")\n",
    "            return np.ones(num_choices) / num_choices\n",
    "\n",
    "    def get_generator_initial_probs(self, question, choices, subject):\n",
    "        \"\"\"Get initial generator probabilities for correct/incorrect.\"\"\"\n",
    "        gen_init = {\"correct\": {}, \"incorrect\": {}}\n",
    "        candidates = [f\"{chr(65+i)}\" for i in range(len(choices))]\n",
    "        \n",
    "        for get_correct in [True, False]:\n",
    "            prompt = self.build_generator_prompt(subject, question, choices, get_correct)\n",
    "            probs = self.get_generator_probabilities(prompt, len(choices))\n",
    "            \n",
    "            for i, candidate in enumerate(candidates):\n",
    "                if get_correct:\n",
    "                    gen_init[\"correct\"][candidate] = probs[i]\n",
    "                else:\n",
    "                    gen_init[\"incorrect\"][candidate] = probs[i]\n",
    "\n",
    "        return gen_init\n",
    "\n",
    "    def build_discriminator_prompt(self, subject, question, proposed_answer):\n",
    "        \"\"\"Build prompt for discriminator.\"\"\"\n",
    "        prompt = f\"\"\"You are an expert evaluator of questions about {self.format_subject(subject)}. \n",
    "Determine if the proposed answer is correct. Output ONLY 'A' or 'B'.\n",
    "Question: {question}\n",
    "Proposed Answer: {proposed_answer}\n",
    "\n",
    "Is this answer correct? Respond ONLY with:\n",
    "A. Correct\n",
    "B. Incorrect\n",
    "\n",
    "Answer:\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def get_discriminator_probabilities(self, prompt_text):\n",
    "        \"\"\"Get discriminator probabilities.\"\"\"\n",
    "        try:\n",
    "            inputs = self.discriminator_tokenizer(prompt_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            \n",
    "            # Move to device if using GPU\n",
    "            if PERF_CONFIG.device == \"cuda\":\n",
    "                inputs = {k: v.to(self.discriminator_model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                with torch.cuda.amp.autocast(enabled=PERF_CONFIG.use_fp16 and PERF_CONFIG.device == \"cuda\"):\n",
    "                    outputs = self.discriminator_model(**inputs)\n",
    "                    logits = outputs.logits[0, -1]\n",
    "\n",
    "            # Get probabilities for A (correct) and B (incorrect)\n",
    "            a_token = self.discriminator_tokenizer.encode(\"A\", add_special_tokens=False)[-1]\n",
    "            b_token = self.discriminator_tokenizer.encode(\"B\", add_special_tokens=False)[-1]\n",
    "            \n",
    "            choice_logits = torch.tensor([logits[a_token], logits[b_token]])\n",
    "            probs = torch.nn.functional.softmax(choice_logits, dim=0).numpy()\n",
    "            \n",
    "            return {\"correct\": float(probs[0]), \"incorrect\": float(probs[1])}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in discriminator probabilities: {e}\")\n",
    "            return {\"correct\": 0.5, \"incorrect\": 0.5}\n",
    "\n",
    "    def get_discriminator_initial_probs(self, question, choices, subject):\n",
    "        \"\"\"Get initial discriminator probabilities for each choice.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for idx, answer in enumerate(choices):\n",
    "            prompt = self.build_discriminator_prompt(subject, question, answer)\n",
    "            probs = self.get_discriminator_probabilities(prompt)\n",
    "            \n",
    "            candidate = f\"{chr(65+idx)}\"\n",
    "            results[candidate] = probs\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def softmax(self, arr):\n",
    "        \"\"\"Numerically stable softmax.\"\"\"\n",
    "        m = np.max(arr)\n",
    "        exp_vals = np.exp(arr - m)\n",
    "        return exp_vals / np.sum(exp_vals)\n",
    "\n",
    "    def equilibrium_search(self, gen_init, disc_init, candidates, T=None, \n",
    "                          eta_G=0.1, eta_D=0.1, lam_G=0.1, lam_D=0.01):\n",
    "        \"\"\"Standard equilibrium search (for compatibility)\"\"\"\n",
    "        if T is None:\n",
    "            T = PERF_CONFIG.equilibrium_iterations\n",
    "            \n",
    "        gen = {\"correct\": dict(gen_init[\"correct\"]), \n",
    "               \"incorrect\": dict(gen_init[\"incorrect\"])}\n",
    "        disc = {}\n",
    "        for y in candidates:\n",
    "            disc[y] = dict(disc_init[y])\n",
    "\n",
    "        Qg = {\"correct\": {y: 0.0 for y in candidates}, \n",
    "              \"incorrect\": {y: 0.0 for y in candidates}}\n",
    "        Qd = {y: {\"correct\": 0.0, \"incorrect\": 0.0} for y in candidates}\n",
    "        \n",
    "        prev_gen = None\n",
    "        prev_disc = None\n",
    "\n",
    "        for t in range(1, T+1):\n",
    "            # Update Q values\n",
    "            for v in [\"correct\", \"incorrect\"]:\n",
    "                for y in candidates:\n",
    "                    Qg[v][y] += (1.0/(2.0*t)) * disc[y][v]\n",
    "\n",
    "            for y in candidates:\n",
    "                for v in [\"correct\", \"incorrect\"]:\n",
    "                    Qd[y][v] += (1.0/(2.0*t)) * gen[v][y]\n",
    "\n",
    "            # Update generator policy\n",
    "            for v in [\"correct\", \"incorrect\"]:\n",
    "                logits = []\n",
    "                for y in candidates:\n",
    "                    val = (Qg[v][y] + lam_G * math.log(gen_init[v][y] + 1e-12)) / (1/eta_G + lam_G)\n",
    "                    logits.append(val)\n",
    "\n",
    "                new_probs = self.softmax(np.array(logits))\n",
    "                for i, y in enumerate(candidates):\n",
    "                    gen[v][y] = new_probs[i]\n",
    "\n",
    "            # Update discriminator policy\n",
    "            for y in candidates:\n",
    "                logits = [\n",
    "                    (Qd[y][\"correct\"] + lam_D * math.log(disc_init[y][\"correct\"] + 1e-12)) / (1/eta_D + lam_D),\n",
    "                    (Qd[y][\"incorrect\"] + lam_D * math.log(disc_init[y][\"incorrect\"] + 1e-12)) / (1/eta_D + lam_D)\n",
    "                ]\n",
    "                probs = self.softmax(np.array(logits))\n",
    "                disc[y][\"correct\"] = probs[0]\n",
    "                disc[y][\"incorrect\"] = probs[1]\n",
    "            \n",
    "            # Early stopping check\n",
    "            if t > 2 and prev_gen is not None:\n",
    "                gen_change = sum(abs(gen[v][y] - prev_gen[v][y]) \n",
    "                               for v in [\"correct\", \"incorrect\"] for y in candidates)\n",
    "                disc_change = sum(abs(disc[y][v] - prev_disc[y][v]) \n",
    "                                for y in candidates for v in [\"correct\", \"incorrect\"])\n",
    "                \n",
    "                max_change = max(gen_change, disc_change) / (len(candidates) * 2)\n",
    "                \n",
    "                if max_change < PERF_CONFIG.early_stop_threshold:\n",
    "                    break\n",
    "            \n",
    "            # Store previous values\n",
    "            prev_gen = {v: dict(gen[v]) for v in [\"correct\", \"incorrect\"]}\n",
    "            prev_disc = {y: dict(disc[y]) for y in candidates}\n",
    "\n",
    "        return gen, disc\n",
    "\n",
    "    def get_final_answers(self, gen_final, disc_final, candidates):\n",
    "        \"\"\"Get final answers from generator and discriminator\"\"\"\n",
    "        # Generator's final answer\n",
    "        gen_answer = None\n",
    "        best_gen_prob = -1.0\n",
    "        for y in candidates:\n",
    "            p = gen_final[\"correct\"][y]\n",
    "            if p > best_gen_prob:\n",
    "                best_gen_prob = p\n",
    "                gen_answer = y\n",
    "        \n",
    "        # Discriminator's final answer\n",
    "        disc_answer = None\n",
    "        best_disc_prob = -1.0\n",
    "        for y in candidates:\n",
    "            p = disc_final[y][\"correct\"]\n",
    "            if p > best_disc_prob:\n",
    "                best_disc_prob = p\n",
    "                disc_answer = y\n",
    "        \n",
    "        return gen_answer, disc_answer\n",
    "\n",
    "#==============================================================================\n",
    "# COMPLETE HYBRID INTEGRATED PIPELINE\n",
    "#==============================================================================\n",
    "\n",
    "class HybridIntegratedPipeline:\n",
    "    \"\"\"\n",
    "    Complete hybrid pipeline: Calibrated base strengths + Softmax game dynamics\n",
    "    This is the main class that orchestrates everything\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, confidence_threshold=0.10, \n",
    "                 calibration_method='comprehensive',\n",
    "                 temperature_strategy='adaptive',\n",
    "                 calibration_size=50):\n",
    "        \n",
    "        self.preprocessing = HybridOptimizedPreprocessingSystem(confidence_threshold)\n",
    "        self.adversarial = HybridOptimizedAdversarialFramework()\n",
    "        self.hybrid_calibrator = None\n",
    "        \n",
    "        self.models_loaded = False\n",
    "        self.calibration_method = calibration_method\n",
    "        self.temperature_strategy = temperature_strategy\n",
    "        self.calibration_size = calibration_size\n",
    "        self.processing_times = []\n",
    "        \n",
    "    def initialize_all_models(self):\n",
    "        \"\"\"Initialize all models and hybrid calibrator.\"\"\"\n",
    "        if not self.models_loaded:\n",
    "            print(\"Initializing hybrid pipeline...\")\n",
    "            \n",
    "            # Load preprocessing models\n",
    "            self.preprocessing.load_preprocessing_models()\n",
    "            \n",
    "            # Set up adversarial framework with same models\n",
    "            self.adversarial.setup_adversarial_models(\n",
    "                self.preprocessing.phi2_model, \n",
    "                self.preprocessing.phi2_tokenizer,\n",
    "                self.preprocessing.qwen2_model, \n",
    "                self.preprocessing.qwen2_tokenizer\n",
    "            )\n",
    "            \n",
    "            # Initialize hybrid calibrator\n",
    "            self.hybrid_calibrator = HybridCalibrationGameSoftmax(\n",
    "                self.preprocessing,\n",
    "                self.adversarial\n",
    "            )\n",
    "            \n",
    "            self.models_loaded = True\n",
    "            print(\"Hybrid pipeline initialized successfully!\")\n",
    "    \n",
    "    def process_single_question(self, question, choices, answer_key, subject):\n",
    "        \"\"\"Process a single question through the hybrid pipeline.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Steps 2-3: Preprocessing\n",
    "        preprocessing_result = self.preprocessing.preprocess_question(question, choices, answer_key)\n",
    "        \n",
    "        filtered_choices = preprocessing_result[\"filtered_choices\"]\n",
    "        new_answer_key = preprocessing_result.get(\"new_answer_key\")\n",
    "        \n",
    "        # Check if only one choice remains (automatic consensus)\n",
    "        if len(filtered_choices) == 1:\n",
    "            consensus_answer = \"A\"\n",
    "            processing_time = time.time() - start_time\n",
    "            self.processing_times.append(processing_time)\n",
    "            \n",
    "            # Calculate consensus accuracy\n",
    "            consensus_accuracy = 0.0\n",
    "            if new_answer_key:\n",
    "                consensus_accuracy = 1.0 if consensus_answer == new_answer_key else 0.0\n",
    "            \n",
    "            return {\n",
    "                **preprocessing_result,\n",
    "                'consensus_achieved': True,\n",
    "                'final_answer': consensus_answer,\n",
    "                'hybrid_method_used': False,\n",
    "                'accuracy': consensus_accuracy,\n",
    "                'processing_time': processing_time\n",
    "            }\n",
    "        \n",
    "        # Steps 4-6: Hybrid Adversarial Training\n",
    "        candidates = [f\"{chr(65+i)}\" for i in range(len(filtered_choices))]\n",
    "        \n",
    "        # Step 4: Get initial probabilities\n",
    "        gen_init = self.adversarial.get_generator_initial_probs(question, filtered_choices, subject)\n",
    "        disc_init = self.adversarial.get_discriminator_initial_probs(question, filtered_choices, subject)\n",
    "        \n",
    "        # Step 5: HYBRID equilibrium search with calibrated strengths + softmax\n",
    "        equilibrium_result = self.hybrid_calibrator.game_theory_softmax_equilibrium(\n",
    "            gen_init, disc_init, candidates,\n",
    "            temperature_strategy=self.temperature_strategy\n",
    "        )\n",
    "        \n",
    "        # Step 6: Get final answer with hybrid weights\n",
    "        final_result = self.hybrid_calibrator.get_final_answer_with_hybrid_weights(\n",
    "            equilibrium_result, candidates, new_answer_key\n",
    "        )\n",
    "        \n",
    "        final_answer = final_result['final_answer']\n",
    "        accuracy = final_result.get('accuracy', 0.0)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        self.processing_times.append(processing_time)\n",
    "        \n",
    "        return {\n",
    "            **preprocessing_result,\n",
    "            'consensus_achieved': False,\n",
    "            'final_answer': final_answer,\n",
    "            'gen_answer': final_result['gen_answer'],\n",
    "            'disc_answer': final_result['disc_answer'],\n",
    "            'hybrid_method_used': True,\n",
    "            'hybrid_weights_used': final_result['weights_used'],\n",
    "            'base_strengths': final_result['base_strengths'],\n",
    "            'weight_evolution': final_result['weight_evolution'],\n",
    "            'equilibrium_history': equilibrium_result['equilibrium_history'],\n",
    "            'convergence_iteration': equilibrium_result['convergence_iteration'],\n",
    "            'temperature_strategy': self.temperature_strategy,\n",
    "            'accuracy': accuracy,\n",
    "            'processing_time': processing_time\n",
    "        }\n",
    "    \n",
    "    def process_dataset(self, df, max_questions=None):\n",
    "        \"\"\"Process complete dataset through hybrid pipeline.\"\"\"\n",
    "        self.initialize_all_models()\n",
    "        \n",
    "        # Phase 1: Hybrid Calibration\n",
    "        calibration_indices = []\n",
    "        print(\"\\nPHASE 1: HYBRID CALIBRATION\")\n",
    "        calibration_results = self.hybrid_calibrator.calibrate_base_strengths(\n",
    "            df, \n",
    "            num_questions=self.calibration_size,\n",
    "            method=self.calibration_method\n",
    "        )\n",
    "        \n",
    "        # Remove calibration questions from main dataset\n",
    "        calibration_indices = df.sample(n=min(self.calibration_size, len(df)), random_state=42).index.tolist()\n",
    "        df = df.drop(calibration_indices)\n",
    "        print(f\"Calibration complete. Removed {len(calibration_indices)} questions from main dataset\")\n",
    "        print(f\"Remaining questions: {len(df)}\")\n",
    "        \n",
    "        # Update preprocessing weights with calibrated base strengths\n",
    "        self.preprocessing.phi2_weight = self.hybrid_calibrator.base_phi2_strength\n",
    "        self.preprocessing.qwen2_weight = self.hybrid_calibrator.base_qwen2_strength\n",
    "        self.preprocessing.weights_calibrated = True\n",
    "        \n",
    "        # Limit to subset if specified\n",
    "        if max_questions is not None:\n",
    "            df = df.head(max_questions)\n",
    "            print(f\"Processing subset of {len(df)} questions...\")\n",
    "        \n",
    "        # Phase 2: Main Processing\n",
    "        results = []\n",
    "        \n",
    "        # Accuracy tracking\n",
    "        total_questions = 0\n",
    "        consensus_questions = 0\n",
    "        hybrid_questions = 0\n",
    "        total_correct = 0\n",
    "        \n",
    "        print(f\"\\nPHASE 2: HYBRID GAME THEORY PROCESSING\")\n",
    "        print(f\"Base strengths: Phi-2={self.hybrid_calibrator.base_phi2_strength:.3f}, Qwen2={self.hybrid_calibrator.base_qwen2_strength:.3f}\")\n",
    "        print(f\"Temperature strategy: {self.temperature_strategy}\")\n",
    "        print(f\"Calibration method: {self.calibration_method}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process each question\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Hybrid pipeline\"):\n",
    "            total_questions += 1\n",
    "            \n",
    "            # Process through hybrid pipeline\n",
    "            result = self.process_single_question(\n",
    "                row[\"question\"], \n",
    "                row[\"choices\"], \n",
    "                row.get(\"answerKey\"),\n",
    "                row[\"subject\"]\n",
    "            )\n",
    "            \n",
    "            # Add original row data\n",
    "            result.update(row.to_dict())\n",
    "            \n",
    "            # Track accuracy\n",
    "            if result['consensus_achieved']:\n",
    "                consensus_questions += 1\n",
    "            else:\n",
    "                hybrid_questions += 1\n",
    "            \n",
    "            accuracy = result.get('accuracy', 0)\n",
    "            if accuracy is not None and accuracy > 0:\n",
    "                total_correct += 1\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Memory management\n",
    "            if total_questions % PERF_CONFIG.clear_cache_every == 0:\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        avg_time_per_question = total_time / total_questions if total_questions > 0 else 0\n",
    "        \n",
    "        # Print comprehensive results\n",
    "        self._print_hybrid_results(\n",
    "            results, total_questions, consensus_questions, hybrid_questions, \n",
    "            total_correct, total_time, avg_time_per_question, calibration_results\n",
    "        )\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def _print_hybrid_results(self, results, total_questions, consensus_questions, hybrid_questions, \n",
    "                             total_correct, total_time, avg_time_per_question, calibration_results):\n",
    "        \"\"\"Print comprehensive hybrid results\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"HYBRID CALIBRATION + GAME SOFTMAX RESULTS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Performance metrics\n",
    "        print(f\"Performance Metrics:\")\n",
    "        print(f\"   Total processing time: {total_time:.2f} seconds\")\n",
    "        print(f\"   Average time per question: {avg_time_per_question:.3f} seconds\")\n",
    "        print(f\"   Questions per second: {total_questions/total_time:.1f}\")\n",
    "        print(f\"   Device used: {PERF_CONFIG.device} ({PERF_CONFIG.device_name})\")\n",
    "        print(f\"   Precision: {'FP16' if PERF_CONFIG.use_fp16 else 'FP32'}\")\n",
    "        \n",
    "        if self.processing_times:\n",
    "            print(f\"   Min processing time: {min(self.processing_times):.3f}s\")\n",
    "            print(f\"   Max processing time: {max(self.processing_times):.3f}s\")\n",
    "        \n",
    "        # Calibration info\n",
    "        print(f\"\\nCalibration Details:\")\n",
    "        print(f\"   Method: {self.calibration_method}\")\n",
    "        print(f\"   Sample size: {self.calibration_size}\")\n",
    "        print(f\"   Phi-2 base strength: {self.hybrid_calibrator.base_phi2_strength:.3f}\")\n",
    "        print(f\"   Qwen2 base strength: {self.hybrid_calibrator.base_qwen2_strength:.3f}\")\n",
    "        print(f\"   Calibration confidence: {self.hybrid_calibrator.calibration_confidence:.3f}\")\n",
    "        \n",
    "        if 'phi2_metrics' in calibration_results:\n",
    "            phi2_metrics = calibration_results['phi2_metrics']\n",
    "            qwen2_metrics = calibration_results['qwen2_metrics']\n",
    "            print(f\"   Phi-2 metrics: Acc={phi2_metrics['accuracy']:.1%}, Conf={phi2_metrics['confidence']:.3f}\")\n",
    "            print(f\"   Qwen2 metrics: Acc={qwen2_metrics['accuracy']:.1%}, Conf={qwen2_metrics['confidence']:.3f}\")\n",
    "            print(f\"   Complementarity: {calibration_results.get('complementarity', 0):.1%}\")\n",
    "        \n",
    "        # Processing breakdown\n",
    "        print(f\"\\nProcessing Breakdown:\")\n",
    "        print(f\"   Total questions: {total_questions}\")\n",
    "        print(f\"   Preprocessing consensus: {consensus_questions} ({consensus_questions/total_questions*100:.1f}%)\")\n",
    "        print(f\"   Hybrid game theory: {hybrid_questions} ({hybrid_questions/total_questions*100:.1f}%)\")\n",
    "        \n",
    "        # Final accuracy (handle None values properly)\n",
    "        valid_results = [r.get('accuracy', 0) for r in results if r.get('accuracy') is not None]\n",
    "        overall_accuracy = sum(valid_results) / len(valid_results) if valid_results else 0\n",
    "        total_correct = sum(1 for acc in valid_results if acc > 0)\n",
    "        \n",
    "        print(f\"\\nFinal Results:\")\n",
    "        print(f\"   Overall accuracy: {total_correct}/{len(valid_results)} = {overall_accuracy:.1%}\")\n",
    "        print(f\"   Valid results: {len(valid_results)}/{total_questions}\")\n",
    "        print(f\"   Temperature strategy: {self.temperature_strategy}\")\n",
    "        \n",
    "        # Temperature analysis\n",
    "        if self.hybrid_calibrator.temperature_history:\n",
    "            temps = self.hybrid_calibrator.temperature_history\n",
    "            print(f\"   Temperature range: {min(temps):.2f} - {max(temps):.2f}\")\n",
    "            print(f\"   Average temperature: {np.mean(temps):.2f}\")\n",
    "        \n",
    "        # Weight evolution analysis\n",
    "        if self.hybrid_calibrator.weight_evolution:\n",
    "            final_weights = self.hybrid_calibrator.weight_evolution[-1]\n",
    "            print(f\"   Final dynamic weights: Gen={final_weights['gen_weight']:.3f}, Disc={final_weights['disc_weight']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nHYBRID PIPELINE COMPLETE!\")\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "    def cleanup_all_models(self):\n",
    "        \"\"\"Clean up all models.\"\"\"\n",
    "        self.preprocessing.cleanup_preprocessing_models()\n",
    "        self.models_loaded = False\n",
    "        self.processing_times = []\n",
    "\n",
    "#==============================================================================\n",
    "# MAIN EXECUTION FUNCTIONS\n",
    "#==============================================================================\n",
    "\n",
    "def test_hybrid_subset(num_questions=50, \n",
    "                      calibration_method='comprehensive',\n",
    "                      temperature_strategy='adaptive'):\n",
    "    \"\"\"Test the hybrid pipeline on a subset\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(f\"TESTING HYBRID PIPELINE - {num_questions} QUESTIONS\")\n",
    "    print(f\"Calibration method: {calibration_method}\")\n",
    "    print(f\"Temperature strategy: {temperature_strategy}\")\n",
    "    print(f\"Device: {PERF_CONFIG.device}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load datasets\n",
    "    arc_df, arc_easy_df = load_arc_datasets()\n",
    "    \n",
    "    # Initialize hybrid pipeline\n",
    "    pipeline = HybridIntegratedPipeline(\n",
    "        confidence_threshold=0.10,\n",
    "        calibration_method=calibration_method,\n",
    "        temperature_strategy=temperature_strategy,\n",
    "        calibration_size=30\n",
    "    )\n",
    "    \n",
    "    # Test on ARC Challenge subset\n",
    "    print(f\"\\nTesting hybrid approach on {num_questions} ARC Challenge questions...\")\n",
    "    results = pipeline.process_dataset(arc_df, max_questions=num_questions)\n",
    "    \n",
    "    # Save results\n",
    "    filename = f\"Data/hybrid_test_{num_questions}_{calibration_method}_{temperature_strategy}_{PERF_CONFIG.device}.csv\"\n",
    "    results.to_csv(filename, index=False)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    pipeline.cleanup_all_models()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_approaches(num_questions=100):\n",
    "    \"\"\"Compare hybrid vs standard approaches\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(f\"COMPARING APPROACHES - {num_questions} QUESTIONS EACH\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load data\n",
    "    arc_df, _ = load_arc_datasets()\n",
    "    test_data = arc_df.head(num_questions + 100)  # Extra for calibration\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test different configurations\n",
    "    configs = [\n",
    "        {'method': 'simple', 'temp': 'fixed', 'name': 'Simple + Fixed Temp'},\n",
    "        {'method': 'simple', 'temp': 'adaptive', 'name': 'Simple + Adaptive Temp'},\n",
    "        {'method': 'comprehensive', 'temp': 'fixed', 'name': 'Comprehensive + Fixed Temp'},\n",
    "        {'method': 'comprehensive', 'temp': 'adaptive', 'name': 'Comprehensive + Adaptive Temp'},\n",
    "        {'method': 'comprehensive', 'temp': 'annealing', 'name': 'Comprehensive + Annealing Temp'},\n",
    "    ]\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\nTesting: {config['name']}\")\n",
    "        \n",
    "        pipeline = HybridIntegratedPipeline(\n",
    "            calibration_method=config['method'],\n",
    "            temperature_strategy=config['temp'],\n",
    "            calibration_size=50\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            config_results = pipeline.process_dataset(test_data, max_questions=num_questions)\n",
    "            \n",
    "            # Handle accuracy calculation properly\n",
    "            accuracies = [r.get('accuracy', 0) for r in config_results.to_dict('records') if r.get('accuracy') is not None]\n",
    "            accuracy = sum(accuracies) / len(accuracies) if accuracies else 0\n",
    "            \n",
    "            results[config['name']] = {\n",
    "                'accuracy': accuracy,\n",
    "                'results': config_results,\n",
    "                'config': config\n",
    "            }\n",
    "            \n",
    "            print(f\"   Accuracy: {accuracy:.1%}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   Error: {e}\")\n",
    "            results[config['name']] = {'accuracy': 0, 'error': str(e)}\n",
    "        \n",
    "        finally:\n",
    "            pipeline.cleanup_all_models()\n",
    "    \n",
    "    # Find best configuration\n",
    "    valid_results = {k: v for k, v in results.items() if 'accuracy' in v and v['accuracy'] > 0}\n",
    "    if valid_results:\n",
    "        best_config = max(valid_results.keys(), key=lambda k: valid_results[k]['accuracy'])\n",
    "        print(f\"\\nBEST CONFIGURATION: {best_config}\")\n",
    "        print(f\"   Accuracy: {valid_results[best_config]['accuracy']:.1%}\")\n",
    "        \n",
    "        # Save best results\n",
    "        best_results = valid_results[best_config]['results']\n",
    "        filename = f\"Data/hybrid_best_config_{num_questions}_{PERF_CONFIG.device}.csv\"\n",
    "        best_results.to_csv(filename, index=False)\n",
    "        print(f\"Best results saved to {filename}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main_hybrid():\n",
    "    \"\"\"Main execution with hybrid pipeline\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"HYBRID ADVERSARIAL QA SYSTEM\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Phase 1: Comprehensive calibration (base model strengths)\")\n",
    "    print(\"Phase 2: Softmax game theory dynamics\")\n",
    "    print(\"Features: Temperature control, weight evolution, adaptive equilibrium\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nChoose execution mode:\")\n",
    "    print(\"1. Quick test (50 questions, comprehensive + adaptive)\")\n",
    "    print(\"2. Compare different configurations (100 questions each)\")\n",
    "    print(\"3. Full ARC Challenge dataset (1000+ questions)\")\n",
    "    print(\"4. Custom configuration test\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1/2/3/4): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        return test_hybrid_subset(50, 'comprehensive', 'adaptive')\n",
    "    \n",
    "    elif choice == \"2\":\n",
    "        return compare_approaches(100)\n",
    "    \n",
    "    elif choice == \"3\":\n",
    "        # Full dataset processing\n",
    "        arc_df, arc_easy_df = load_arc_datasets()\n",
    "        \n",
    "        pipeline = HybridIntegratedPipeline(\n",
    "            calibration_method='comprehensive',\n",
    "            temperature_strategy='adaptive',\n",
    "            calibration_size=100\n",
    "        )\n",
    "        \n",
    "        print(\"\\nProcessing FULL ARC Challenge dataset with hybrid approach...\")\n",
    "        results_challenge = pipeline.process_dataset(arc_df)\n",
    "        \n",
    "        # Save results\n",
    "        challenge_filename = f\"Data/hybrid_full_arc_challenge_{PERF_CONFIG.device}.csv\"\n",
    "        results_challenge.to_csv(challenge_filename, index=False)\n",
    "        print(f\"ARC Challenge results saved to {challenge_filename}\")\n",
    "        \n",
    "        print(\"\\nProcessing FULL ARC Easy dataset with hybrid approach...\")\n",
    "        results_easy = pipeline.process_dataset(arc_easy_df)\n",
    "        \n",
    "        # Save results\n",
    "        easy_filename = f\"Data/hybrid_full_arc_easy_{PERF_CONFIG.device}.csv\"\n",
    "        results_easy.to_csv(easy_filename, index=False)\n",
    "        print(f\"ARC Easy results saved to {easy_filename}\")\n",
    "        \n",
    "        # Final cleanup\n",
    "        pipeline.cleanup_all_models()\n",
    "        \n",
    "        print(f\"\\nCOMPLETE HYBRID PROCESSING FINISHED!\")\n",
    "        return results_challenge, results_easy\n",
    "    \n",
    "    elif choice == \"4\":\n",
    "        # Custom configuration\n",
    "        print(\"\\nCustom Configuration:\")\n",
    "        \n",
    "        cal_method = input(\"Calibration method (simple/comprehensive) [comprehensive]: \").strip() or 'comprehensive'\n",
    "        temp_strategy = input(\"Temperature strategy (fixed/adaptive/annealing) [adaptive]: \").strip() or 'adaptive'\n",
    "        num_questions = int(input(\"Number of test questions [100]: \").strip() or '100')\n",
    "        \n",
    "        return test_hybrid_subset(num_questions, cal_method, temp_strategy)\n",
    "    \n",
    "    else:\n",
    "        print(\"Invalid choice. Running quick test...\")\n",
    "        return test_hybrid_subset(50, 'comprehensive', 'adaptive')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_hybrid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

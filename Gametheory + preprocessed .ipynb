{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e07ad53-062c-4887-9ac3-4297c58ab015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ADVERSARIAL QA SYSTEM - SWAPPED ROLES\n",
      "============================================================\n",
      "Step 1: Load ARC datasets\n",
      "Step 2: Phi-2 + Qwen2-1.5B â†’ confidence analysis\n",
      "Step 3: Filter choices with prob < threshold\n",
      "Step 4: Phi-2 (Generator) vs Qwen2-1.5B (Discriminator)\n",
      "Step 5: Nash equilibrium search\n",
      "Step 6: Final answers\n",
      "============================================================\n",
      "\n",
      "Choose execution mode:\n",
      "1. Test on subset (20 questions) - RECOMMENDED FIRST\n",
      "2. Test on larger subset (100 questions)\n",
      "3. Full dataset (1000+ questions)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1/2/3):  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ARC Challenge dataset...\n",
      "ARC Challenge shape: (1170, 5)\n",
      "Loading ARC Easy dataset...\n",
      "ARC Easy shape: (2371, 5)\n",
      "\n",
      "Processing FULL ARC Challenge dataset...\n",
      "Initializing complete pipeline...\n",
      "Loading Phi-2 for preprocessing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d653026ad8f4beeb2b17be93be2c9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen2-1.5B for preprocessing...\n",
      "Preprocessing models loaded successfully!\n",
      "Adversarial roles set:\n",
      "  Generator: Phi-2\n",
      "  Discriminator: Qwen2-1.5B-Instruct\n",
      "All models initialized successfully!\n",
      "Processing dataset through complete pipeline...\n",
      "Pipeline: Steps 1-6\n",
      "  1. Load questions\n",
      "  2. Phi-2 + Qwen2-1.5B â†’ confidence scores\n",
      "  3. Filter choices with prob < 0.10\n",
      "  4. Qwen2-1.5B (Generator) vs Phi-2 (Discriminator)\n",
      "  5. Nash equilibrium search\n",
      "  6. Final answers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   0%|                  | 1/1170 [01:31<29:44:49, 91.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus: Mercury_7175875 - Correct: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   0%|                  | 2/1170 [02:38<25:03:34, 77.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus: Mercury_SC_409171 - Correct: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   0%|                 | 3/1170 [04:57<34:08:11, 105.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_SC_408547 - Gen: True (Î”+0.0), Disc: True (Î”+1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   0%|                  | 4/1170 [06:06<29:25:09, 90.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus: Mercury_407327 - Correct: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   0%|                 | 5/1170 [08:28<35:24:26, 109.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: MCAS_2006_9_44 - Gen: False (Î”+0.0), Disc: False (Î”+0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   1%|                 | 6/1170 [11:04<40:33:05, 125.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_7270393 - Gen: False (Î”-1.0), Disc: False (Î”+0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   1%|                 | 7/1170 [14:16<47:27:07, 146.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: MCAS_2014_5_7 - Gen: True (Î”+0.0), Disc: True (Î”+1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   1%|                 | 8/1170 [17:18<51:04:11, 158.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_7086660 - Gen: False (Î”+0.0), Disc: False (Î”-1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   1%|â–                | 9/1170 [20:27<54:06:49, 167.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_7168805 - Gen: False (Î”-1.0), Disc: True (Î”+1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   1%|â–               | 10/1170 [23:09<53:31:53, 166.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: MCAS_2003_8_11 - Gen: True (Î”+0.0), Disc: True (Î”+0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   1%|â–               | 11/1170 [25:38<51:46:28, 160.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_7250058 - Gen: True (Î”+0.0), Disc: True (Î”+1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   1%|â–               | 12/1170 [28:19<51:42:42, 160.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_7012740 - Gen: False (Î”+0.0), Disc: False (Î”-1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   1%|â–               | 13/1170 [31:42<55:45:52, 173.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_LBS10610 - Gen: True (Î”+0.0), Disc: True (Î”+1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   1%|â–               | 14/1170 [34:44<56:34:02, 176.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_SC_407400 - Gen: True (Î”+0.0), Disc: True (Î”+0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   1%|â–               | 15/1170 [37:50<57:30:02, 179.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_7212993 - Gen: False (Î”-1.0), Disc: True (Î”+1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   1%|â–               | 16/1170 [40:24<54:59:24, 171.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_SC_413240 - Gen: True (Î”+0.0), Disc: True (Î”+0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   1%|â–               | 17/1170 [41:37<45:25:27, 141.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus: Mercury_7186358 - Correct: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   2%|â–               | 18/1170 [43:57<45:14:53, 141.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_7166425 - Gen: True (Î”+0.0), Disc: True (Î”+0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   2%|â–Ž               | 19/1170 [45:05<38:07:10, 119.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus: MDSA_2007_8_3 - Correct: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   2%|â–Ž               | 20/1170 [48:27<46:05:47, 144.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_7094290 - Gen: True (Î”+0.0), Disc: True (Î”+0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   2%|â–Ž               | 21/1170 [49:38<39:00:12, 122.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus: Mercury_7186568 - Correct: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   2%|â–Ž               | 22/1170 [51:10<36:04:15, 113.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus: Mercury_402216 - Correct: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   2%|â–Ž               | 23/1170 [55:01<47:20:54, 148.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_404894 - Gen: True (Î”+0.0), Disc: True (Î”+0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   2%|â–Ž               | 24/1170 [58:28<52:52:59, 166.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: MCAS_2002_8_11 - Gen: True (Î”+1.0), Disc: False (Î”-1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   2%|â–Ž             | 25/1170 [1:00:44<49:57:32, 157.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_SC_405086 - Gen: True (Î”+0.0), Disc: True (Î”+0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   2%|â–Ž             | 26/1170 [1:03:06<48:28:04, 152.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_SC_408324 - Gen: True (Î”+1.0), Disc: True (Î”+0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   2%|â–Ž             | 27/1170 [1:05:32<47:45:44, 150.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_7218820 - Gen: False (Î”+0.0), Disc: False (Î”+0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   2%|â–Ž             | 28/1170 [1:08:06<48:04:09, 151.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_412202 - Gen: True (Î”+0.0), Disc: True (Î”+1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   2%|â–Ž             | 29/1170 [1:10:14<45:50:03, 144.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus: Mercury_SC_409139 - Correct: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   3%|â–Ž             | 30/1170 [1:13:18<49:32:42, 156.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus: Mercury_400687 - Correct: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   3%|â–Ž             | 31/1170 [1:16:47<54:29:52, 172.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_7171605 - Gen: True (Î”+0.0), Disc: True (Î”+1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   3%|â–             | 32/1170 [1:19:51<55:32:15, 175.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial: Mercury_7210245 - Gen: False (Î”+0.0), Disc: False (Î”-1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pipeline:   3%|â–             | 33/1170 [1:21:00<45:23:30, 143.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consensus: AKDE&ED_2008_4_25 - Correct: True\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Set up HuggingFace authentication\n",
    "#os.environ[\"HF_TOKEN\"] = \"hf_***REDACTED***\"\n",
    "#login(os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "#==============================================================================\n",
    "# STEP 1: DATA LOADING AND SETUP\n",
    "#==============================================================================\n",
    "\n",
    "def load_arc_datasets():\n",
    "    \"\"\"Load both ARC Challenge and Easy datasets.\"\"\"\n",
    "    print(\"Loading ARC Challenge dataset...\")\n",
    "    arc_data = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
    "    arc_df = arc_data.to_pandas()\n",
    "    arc_df = arc_df.drop_duplicates(subset=['question'])\n",
    "    arc_df[\"choices\"] = arc_df[\"choices\"].apply(lambda x: x[\"text\"])\n",
    "    arc_df[\"subject\"] = \"science\"\n",
    "    print(f\"ARC Challenge shape: {arc_df.shape}\")\n",
    "    \n",
    "    print(\"Loading ARC Easy dataset...\")\n",
    "    arc_easy_data = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\", split=\"test\")\n",
    "    arc_easy_df = arc_easy_data.to_pandas()\n",
    "    arc_easy_df = arc_easy_df.drop_duplicates(subset=['question'])\n",
    "    arc_easy_df[\"choices\"] = arc_easy_df[\"choices\"].apply(lambda x: x[\"text\"])\n",
    "    arc_easy_df[\"subject\"] = \"science\"\n",
    "    print(f\"ARC Easy shape: {arc_easy_df.shape}\")\n",
    "    \n",
    "    return arc_df, arc_easy_df\n",
    "\n",
    "#==============================================================================\n",
    "# STEP 2: PREPROCESSING SYSTEM (Phi-2 + Qwen2-1.5B for Confidence Filtering)\n",
    "#==============================================================================\n",
    "\n",
    "class PreprocessingSystem:\n",
    "    \"\"\"\n",
    "    Step 2: Use Phi-2 + Qwen2-1.5B to analyze all choices and filter out low-confidence options\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, confidence_threshold=0.10):\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.phi2_model = None\n",
    "        self.phi2_tokenizer = None\n",
    "        self.qwen2_model = None\n",
    "        self.qwen2_tokenizer = None\n",
    "        \n",
    "    def load_preprocessing_models(self):\n",
    "        \"\"\"Load both models for preprocessing.\"\"\"\n",
    "        print(\"Loading Phi-2 for preprocessing...\")\n",
    "        self.phi2_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"microsoft/phi-2\",\n",
    "            torch_dtype=torch.float32,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"cpu\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.phi2_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"microsoft/phi-2\", \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        if self.phi2_tokenizer.pad_token is None:\n",
    "            self.phi2_tokenizer.pad_token = self.phi2_tokenizer.eos_token\n",
    "            \n",
    "        print(\"Loading Qwen2-1.5B for preprocessing...\")\n",
    "        self.qwen2_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "            torch_dtype=torch.float32,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"cpu\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.qwen2_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        if self.qwen2_tokenizer.pad_token is None:\n",
    "            self.qwen2_tokenizer.pad_token = self.qwen2_tokenizer.eos_token\n",
    "            \n",
    "        print(\"Preprocessing models loaded successfully!\")\n",
    "        \n",
    "    def build_analysis_prompt(self, question, choices):\n",
    "        \"\"\"Build prompt for confidence analysis.\"\"\"\n",
    "        prompt = f\"\"\"The following is a multiple choice science question. Analyze all choices and select the most likely correct answer.\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "        for i, choice in enumerate(choices):\n",
    "            prompt += f\"{chr(65+i)}. {choice}\\n\"\n",
    "        prompt += \"\\nAnswer:\"\n",
    "        return prompt\n",
    "        \n",
    "    def get_model_confidence_scores(self, model, tokenizer, prompt, num_choices):\n",
    "        \"\"\"Get confidence scores from a model for answer choices.\"\"\"\n",
    "        try:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits[0, -1]\n",
    "            \n",
    "            # Get probabilities for A, B, C, D based on number of choices\n",
    "            choice_logits = []\n",
    "            for i in range(num_choices):\n",
    "                letter = chr(65 + i)  # A, B, C, D\n",
    "                token_id = tokenizer.encode(letter, add_special_tokens=False)[-1]\n",
    "                choice_logits.append(logits[token_id].item())\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            choice_logits = torch.tensor(choice_logits)\n",
    "            probs = torch.nn.functional.softmax(choice_logits, dim=0).numpy()\n",
    "            \n",
    "            return probs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting confidence scores: {e}\")\n",
    "            # Return uniform distribution as fallback\n",
    "            return np.ones(num_choices) / num_choices\n",
    "    \n",
    "    def analyze_all_choices(self, question, choices):\n",
    "        \"\"\"\n",
    "        Step 2: Phi-2 + Qwen2-1.5B analyze all choices â†’ combined probabilities\n",
    "        \"\"\"\n",
    "        prompt = self.build_analysis_prompt(question, choices)\n",
    "        num_choices = len(choices)\n",
    "        \n",
    "        # Get confidence scores from both models\n",
    "        phi2_probs = self.get_model_confidence_scores(\n",
    "            self.phi2_model, self.phi2_tokenizer, prompt, num_choices\n",
    "        )\n",
    "        qwen2_probs = self.get_model_confidence_scores(\n",
    "            self.qwen2_model, self.qwen2_tokenizer, prompt, num_choices\n",
    "        )\n",
    "        \n",
    "        # Combined with 50/50 weighting\n",
    "        combined_probs = 0.5 * phi2_probs + 0.5 * qwen2_probs\n",
    "        \n",
    "        return {\n",
    "            'phi2_probs': phi2_probs,\n",
    "            'qwen2_probs': qwen2_probs,\n",
    "            'combined_probs': combined_probs\n",
    "        }\n",
    "    \n",
    "    def filter_low_confidence_choices(self, choices, combined_probs):\n",
    "        \"\"\"\n",
    "        Step 3: Filter out choices with prob < 0.10 threshold\n",
    "        \"\"\"\n",
    "        # Find choices above threshold\n",
    "        high_confidence_indices = [\n",
    "            i for i, prob in enumerate(combined_probs) \n",
    "            if prob >= self.confidence_threshold\n",
    "        ]\n",
    "        \n",
    "        # Fallback: keep all choices if none meet threshold\n",
    "        if not high_confidence_indices:\n",
    "            high_confidence_indices = list(range(len(choices)))\n",
    "        \n",
    "        # Extract high-confidence choices\n",
    "        filtered_choices = [choices[i] for i in high_confidence_indices]\n",
    "        filtered_probs = [combined_probs[i] for i in high_confidence_indices]\n",
    "        \n",
    "        return filtered_choices, filtered_probs, high_confidence_indices\n",
    "    \n",
    "    def randomize_choice_order(self, choices, probs, original_indices):\n",
    "        \"\"\"Randomly shuffle choices to prevent positional bias.\"\"\"\n",
    "        combined = list(zip(choices, probs, original_indices))\n",
    "        random.shuffle(combined)\n",
    "        \n",
    "        shuffled_choices, shuffled_probs, shuffled_original_indices = zip(*combined)\n",
    "        position_mapping = {i: orig_idx for i, orig_idx in enumerate(shuffled_original_indices)}\n",
    "        \n",
    "        return list(shuffled_choices), list(shuffled_probs), position_mapping\n",
    "    \n",
    "    def preprocess_question(self, question, choices, answer_key=None):\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline: Steps 2-3\n",
    "        \"\"\"\n",
    "        # Step 2: Analyze all choices\n",
    "        analysis_results = self.analyze_all_choices(question, choices)\n",
    "        \n",
    "        # Step 3: Filter low-confidence choices\n",
    "        filtered_choices, filtered_probs, high_confidence_indices = self.filter_low_confidence_choices(\n",
    "            choices, analysis_results['combined_probs']\n",
    "        )\n",
    "        \n",
    "        # Randomize to prevent bias\n",
    "        final_choices, final_probs, position_mapping = self.randomize_choice_order(\n",
    "            filtered_choices, filtered_probs, high_confidence_indices\n",
    "        )\n",
    "        \n",
    "        # Update answer key if provided\n",
    "        new_answer_key = None\n",
    "        if answer_key:\n",
    "            original_answer_idx = ord(answer_key) - ord('A')\n",
    "            if original_answer_idx in high_confidence_indices:\n",
    "                for new_pos, orig_pos in position_mapping.items():\n",
    "                    if orig_pos == original_answer_idx:\n",
    "                        new_answer_key = chr(ord('A') + new_pos)\n",
    "                        break\n",
    "        \n",
    "        return {\n",
    "            'original_question': question,\n",
    "            'original_choices': choices,\n",
    "            'filtered_choices': final_choices,\n",
    "            'analysis_results': analysis_results,\n",
    "            'position_mapping': position_mapping,\n",
    "            'high_confidence_indices': high_confidence_indices,\n",
    "            'original_answer_key': answer_key,\n",
    "            'new_answer_key': new_answer_key,\n",
    "            'filtering_applied': len(final_choices) < len(choices)\n",
    "        }\n",
    "    \n",
    "    def cleanup_preprocessing_models(self):\n",
    "        \"\"\"Clean up preprocessing models.\"\"\"\n",
    "        if self.phi2_model:\n",
    "            del self.phi2_model\n",
    "        if self.phi2_tokenizer:\n",
    "            del self.phi2_tokenizer\n",
    "        if self.qwen2_model:\n",
    "            del self.qwen2_model\n",
    "        if self.qwen2_tokenizer:\n",
    "            del self.qwen2_tokenizer\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "#==============================================================================\n",
    "# STEP 4: ADVERSARIAL FRAMEWORK (Qwen2-1.5B Generator vs Phi-2 Discriminator)\n",
    "#==============================================================================\n",
    "\n",
    "class AdversarialFramework:\n",
    "    \"\"\"\n",
    "    Steps 4-6: Adversarial training between Qwen2-1.5B (Generator) and Phi-2 (Discriminator)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.generator_model = None      # Qwen2-1.5B\n",
    "        self.generator_tokenizer = None\n",
    "        self.discriminator_model = None  # Phi-2\n",
    "        self.discriminator_tokenizer = None\n",
    "    \n",
    "    def setup_adversarial_models(self, phi2_model, phi2_tokenizer, qwen2_model, qwen2_tokenizer):\n",
    "        \"\"\"\n",
    "        Step 4: Set up adversarial roles\n",
    "        Generator: Phi-2 (tries to pick correct answers)\n",
    "        Discriminator: Qwen2-1.5B (tries to evaluate correctness)\n",
    "        \"\"\"\n",
    "        self.generator_model = phi2_model         # Phi-2 as Generator\n",
    "        self.generator_tokenizer = phi2_tokenizer\n",
    "        self.discriminator_model = qwen2_model    # Qwen2-1.5B as Discriminator\n",
    "        self.discriminator_tokenizer = qwen2_tokenizer\n",
    "        \n",
    "        print(\"Adversarial roles set:\")\n",
    "        print(\"  Generator: Phi-2\")\n",
    "        print(\"  Discriminator: Qwen2-1.5B-Instruct\")\n",
    "\n",
    "    def format_subject(self, subject):\n",
    "        \"\"\"Format subject string.\"\"\"\n",
    "        return \" \".join(subject.split(\"_\"))\n",
    "\n",
    "    def build_generator_prompt(self, subject, question, choices, get_correct):\n",
    "        \"\"\"Build prompt for generator (Qwen2-1.5B).\"\"\"\n",
    "        prompt = f\"The following are multiple choice questions (with answers) about {self.format_subject(subject)}.\\n\\n\"\n",
    "        prompt += f\"{question}\"\n",
    "        \n",
    "        for i, choice in enumerate(choices):\n",
    "            prompt += f\"\\n{chr(65+i)}. {choice}\"\n",
    "            \n",
    "        if get_correct:\n",
    "            prompt += \"\\nAnswer:\"\n",
    "        else:\n",
    "            prompt += \"\\nIncorrect Answer:\"\n",
    "        return prompt\n",
    "\n",
    "    def get_generator_probabilities(self, prompt_text, num_choices):\n",
    "        \"\"\"Get generator (Qwen2-1.5B) answer probabilities.\"\"\"\n",
    "        try:\n",
    "            input_ids = self.generator_tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(self.generator_model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = self.generator_model(input_ids=input_ids).logits[0, -1]\n",
    "\n",
    "            # Get probabilities for A, B, C, D...\n",
    "            choice_logits = []\n",
    "            for i in range(num_choices):\n",
    "                letter = chr(65 + i)\n",
    "                token_id = self.generator_tokenizer(letter, return_tensors=\"pt\").input_ids[0, -1].item()\n",
    "                choice_logits.append(logits[token_id].item())\n",
    "            \n",
    "            choice_logits = torch.tensor(choice_logits, device=self.generator_model.device).float()\n",
    "            probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
    "            \n",
    "            return probs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in generator probabilities: {e}\")\n",
    "            return np.ones(num_choices) / num_choices\n",
    "\n",
    "    def get_generator_initial_probs(self, question, choices, subject):\n",
    "        \"\"\"Get initial generator probabilities for correct/incorrect.\"\"\"\n",
    "        gen_init = {\"correct\": {}, \"incorrect\": {}}\n",
    "        candidates = [f\"{chr(65+i)}\" for i in range(len(choices))]\n",
    "        \n",
    "        for get_correct in [True, False]:\n",
    "            prompt = self.build_generator_prompt(subject, question, choices, get_correct)\n",
    "            probs = self.get_generator_probabilities(prompt, len(choices))\n",
    "            \n",
    "            for i, candidate in enumerate(candidates):\n",
    "                if get_correct:\n",
    "                    gen_init[\"correct\"][candidate] = probs[i]\n",
    "                else:\n",
    "                    gen_init[\"incorrect\"][candidate] = probs[i]\n",
    "\n",
    "        return gen_init\n",
    "\n",
    "    def build_discriminator_prompt(self, subject, question, proposed_answer):\n",
    "        \"\"\"Build prompt for discriminator (Phi-2).\"\"\"\n",
    "        prompt = f\"\"\"You are an expert evaluator of questions about {self.format_subject(subject)}. \n",
    "Determine if the proposed answer is correct. Output ONLY 'A' or 'B'.\n",
    "Question: {question}\n",
    "Proposed Answer: {proposed_answer}\n",
    "\n",
    "Is this answer correct? Respond ONLY with:\n",
    "A. Correct\n",
    "B. Incorrect\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def get_discriminator_probabilities(self, prompt_text):\n",
    "        \"\"\"Get discriminator (Phi-2) probabilities.\"\"\"\n",
    "        try:\n",
    "            input_ids = self.discriminator_tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(self.discriminator_model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = self.discriminator_model(input_ids=input_ids).logits[0, -1]\n",
    "\n",
    "            choice_logits = torch.tensor([\n",
    "                logits[self.discriminator_tokenizer(\"A\").input_ids[-1]],\n",
    "                logits[self.discriminator_tokenizer(\"B\").input_ids[-1]],\n",
    "            ]).float()\n",
    "            \n",
    "            probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
    "            \n",
    "            return {\"correct\": probs[0], \"incorrect\": probs[1]}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in discriminator probabilities: {e}\")\n",
    "            return {\"correct\": 0.5, \"incorrect\": 0.5}\n",
    "\n",
    "    def get_discriminator_initial_probs(self, question, choices, subject):\n",
    "        \"\"\"Get initial discriminator probabilities for each choice.\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for idx, answer in enumerate(choices):\n",
    "            prompt = self.build_discriminator_prompt(subject, question, answer)\n",
    "            probs = self.get_discriminator_probabilities(prompt)\n",
    "            \n",
    "            candidate = f\"{chr(65+idx)}\"\n",
    "            results[candidate] = probs\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def softmax(self, arr):\n",
    "        \"\"\"Numerically stable softmax.\"\"\"\n",
    "        m = np.max(arr)\n",
    "        exp_vals = np.exp(arr - m)\n",
    "        return exp_vals / np.sum(exp_vals)\n",
    "\n",
    "    def equilibrium_search(self, gen_init, disc_init, candidates, T=5000, \n",
    "                          eta_G=0.1, eta_D=0.1, lam_G=0.1, lam_D=0.01):\n",
    "        \"\"\"\n",
    "        Step 5: Equilibrium search finds Nash equilibrium\n",
    "        \"\"\"\n",
    "        gen = {\"correct\": dict(gen_init[\"correct\"]), \n",
    "               \"incorrect\": dict(gen_init[\"incorrect\"])}\n",
    "        disc = {}\n",
    "        for y in candidates:\n",
    "            disc[y] = dict(disc_init[y])\n",
    "\n",
    "        Qg = {\"correct\": {y: 0.0 for y in candidates}, \n",
    "              \"incorrect\": {y: 0.0 for y in candidates}}\n",
    "        Qd = {y: {\"correct\": 0.0, \"incorrect\": 0.0} for y in candidates}\n",
    "\n",
    "        for t in range(1, T+1):\n",
    "            # Update Q values\n",
    "            for v in [\"correct\", \"incorrect\"]:\n",
    "                for y in candidates:\n",
    "                    Qg[v][y] += (1.0/(2.0*t)) * disc[y][v]\n",
    "\n",
    "            for y in candidates:\n",
    "                for v in [\"correct\", \"incorrect\"]:\n",
    "                    Qd[y][v] += (1.0/(2.0*t)) * gen[v][y]\n",
    "\n",
    "            # Update generator policy\n",
    "            for v in [\"correct\", \"incorrect\"]:\n",
    "                logits = []\n",
    "                for y in candidates:\n",
    "                    val = (Qg[v][y] + lam_G * math.log(gen_init[v][y] + 1e-12)) / (1/eta_G + lam_G)\n",
    "                    logits.append(val)\n",
    "\n",
    "                new_probs = self.softmax(np.array(logits))\n",
    "                for i, y in enumerate(candidates):\n",
    "                    gen[v][y] = new_probs[i]\n",
    "\n",
    "            # Update discriminator policy\n",
    "            logits_correct = []\n",
    "            logits_incorrect = []\n",
    "            for y in candidates:\n",
    "                val_correct = (Qd[y][\"correct\"] + lam_D * math.log(disc_init[y][\"correct\"] + 1e-12)) / (1/eta_D + lam_D)\n",
    "                logits_correct.append(val_correct)\n",
    "\n",
    "                val_incorrect = (Qd[y][\"incorrect\"] + lam_D * math.log(disc_init[y][\"incorrect\"] + 1e-12)) / (1/eta_D + lam_D)\n",
    "                logits_incorrect.append(val_incorrect)\n",
    "\n",
    "            new_probs_correct = self.softmax(np.array(logits_correct))\n",
    "            new_probs_incorrect = self.softmax(np.array(logits_incorrect))\n",
    "\n",
    "            for i, y in enumerate(candidates):\n",
    "                disc[y][\"correct\"] = new_probs_correct[i]\n",
    "                disc[y][\"incorrect\"] = new_probs_incorrect[i]\n",
    "\n",
    "        return gen, disc\n",
    "\n",
    "    def get_final_answers(self, gen_final, disc_final, candidates):\n",
    "        \"\"\"\n",
    "        Step 6: Final answers from Generator (Qwen2) vs Discriminator (Phi-2)\n",
    "        \"\"\"\n",
    "        # Generator's final answer\n",
    "        gen_answer = None\n",
    "        best_gen_prob = -1.0\n",
    "        for y in candidates:\n",
    "            p = gen_final[\"correct\"][y]\n",
    "            if p > best_gen_prob:\n",
    "                best_gen_prob = p\n",
    "                gen_answer = y\n",
    "        \n",
    "        # Discriminator's final answer\n",
    "        disc_answer = None\n",
    "        best_disc_prob = -1.0\n",
    "        for y in candidates:\n",
    "            p = disc_final[y][\"correct\"]\n",
    "            if p > best_disc_prob:\n",
    "                best_disc_prob = p\n",
    "                disc_answer = y\n",
    "        \n",
    "        return gen_answer, disc_answer\n",
    "\n",
    "#==============================================================================\n",
    "# STEP 7: INTEGRATED PIPELINE\n",
    "#==============================================================================\n",
    "\n",
    "class IntegratedPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline combining preprocessing + adversarial training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, confidence_threshold=0.10):\n",
    "        self.preprocessing = PreprocessingSystem(confidence_threshold)\n",
    "        self.adversarial = AdversarialFramework()\n",
    "        self.models_loaded = False\n",
    "        \n",
    "    def initialize_all_models(self):\n",
    "        \"\"\"Initialize all models.\"\"\"\n",
    "        if not self.models_loaded:\n",
    "            print(\"Initializing complete pipeline...\")\n",
    "            \n",
    "            # Load preprocessing models\n",
    "            self.preprocessing.load_preprocessing_models()\n",
    "            \n",
    "            # Set up adversarial framework with same models\n",
    "            self.adversarial.setup_adversarial_models(\n",
    "                self.preprocessing.phi2_model, \n",
    "                self.preprocessing.phi2_tokenizer,\n",
    "                self.preprocessing.qwen2_model, \n",
    "                self.preprocessing.qwen2_tokenizer\n",
    "            )\n",
    "            \n",
    "            self.models_loaded = True\n",
    "            print(\"All models initialized successfully!\")\n",
    "        \n",
    "    def evaluate_policy_accuracy(self, policy_probs, correct_answer, candidates, policy_type):\n",
    "        \"\"\"Evaluate accuracy of a policy (initial or final).\"\"\"\n",
    "        if policy_type == \"generator\":\n",
    "            # For generator, use \"correct\" probabilities\n",
    "            best_answer = max(candidates, key=lambda x: policy_probs[\"correct\"][x])\n",
    "        else:\n",
    "            # For discriminator, use \"correct\" probabilities for each choice\n",
    "            best_answer = max(candidates, key=lambda x: policy_probs[x][\"correct\"])\n",
    "        \n",
    "        return 1.0 if best_answer == correct_answer else 0.0\n",
    "\n",
    "    def process_single_question(self, question, choices, answer_key, subject):\n",
    "        \"\"\"Process a single question through the complete pipeline.\"\"\"\n",
    "        \n",
    "        # Steps 2-3: Preprocessing\n",
    "        preprocessing_result = self.preprocessing.preprocess_question(question, choices, answer_key)\n",
    "        \n",
    "        filtered_choices = preprocessing_result[\"filtered_choices\"]\n",
    "        new_answer_key = preprocessing_result.get(\"new_answer_key\")\n",
    "        \n",
    "        # Check if only one choice remains (automatic consensus)\n",
    "        if len(filtered_choices) == 1:\n",
    "            consensus_answer = \"A\"\n",
    "            return {\n",
    "                **preprocessing_result,\n",
    "                'consensus_achieved': True,\n",
    "                'gen_final_answer': consensus_answer,\n",
    "                'disc_final_answer': consensus_answer,\n",
    "                'adversarial_training_applied': False,\n",
    "                'preprocessing_eliminated_choices': len(choices) - 1,\n",
    "                'gen_initial_accuracy': None,\n",
    "                'disc_initial_accuracy': None,\n",
    "                'gen_final_accuracy': 1.0 if new_answer_key == \"A\" else 0.0,\n",
    "                'disc_final_accuracy': 1.0 if new_answer_key == \"A\" else 0.0,\n",
    "                'gen_accuracy_change': None,\n",
    "                'disc_accuracy_change': None\n",
    "            }\n",
    "        \n",
    "        # Steps 4-6: Adversarial training\n",
    "        candidates = [f\"{chr(65+i)}\" for i in range(len(filtered_choices))]\n",
    "        \n",
    "        # Step 4: Get initial probabilities\n",
    "        gen_init = self.adversarial.get_generator_initial_probs(question, filtered_choices, subject)\n",
    "        disc_init = self.adversarial.get_discriminator_initial_probs(question, filtered_choices, subject)\n",
    "        \n",
    "        # Evaluate initial accuracy BEFORE equilibrium search\n",
    "        gen_initial_accuracy = self.evaluate_policy_accuracy(gen_init, new_answer_key, candidates, \"generator\")\n",
    "        disc_initial_accuracy = self.evaluate_policy_accuracy(disc_init, new_answer_key, candidates, \"discriminator\")\n",
    "        \n",
    "        # Step 5: Equilibrium search\n",
    "        gen_final, disc_final = self.adversarial.equilibrium_search(\n",
    "            gen_init, disc_init, candidates, T=20, \n",
    "            eta_G=0.1, eta_D=0.1, lam_G=0.1, lam_D=0.1\n",
    "        )\n",
    "        \n",
    "        # Step 6: Get final answers\n",
    "        gen_answer, disc_answer = self.adversarial.get_final_answers(gen_final, disc_final, candidates)\n",
    "        \n",
    "        # Evaluate final accuracy AFTER equilibrium search\n",
    "        gen_final_accuracy = 1.0 if gen_answer == new_answer_key else 0.0\n",
    "        disc_final_accuracy = 1.0 if disc_answer == new_answer_key else 0.0\n",
    "        \n",
    "        # Calculate accuracy change due to game theory\n",
    "        gen_accuracy_change = gen_final_accuracy - gen_initial_accuracy\n",
    "        disc_accuracy_change = disc_final_accuracy - disc_initial_accuracy\n",
    "        \n",
    "        return {\n",
    "            **preprocessing_result,\n",
    "            'consensus_achieved': False,\n",
    "            'gen_final_answer': gen_answer,\n",
    "            'disc_final_answer': disc_answer,\n",
    "            'adversarial_training_applied': True,\n",
    "            'gen_init_probs': gen_init,\n",
    "            'disc_init_probs': disc_init,\n",
    "            'gen_final_probs': gen_final,\n",
    "            'disc_final_probs': disc_final,\n",
    "            'preprocessing_eliminated_choices': len(choices) - len(filtered_choices),\n",
    "            'gen_initial_accuracy': gen_initial_accuracy,\n",
    "            'disc_initial_accuracy': disc_initial_accuracy,\n",
    "            'gen_final_accuracy': gen_final_accuracy,\n",
    "            'disc_final_accuracy': disc_final_accuracy,\n",
    "            'gen_accuracy_change': gen_accuracy_change,\n",
    "            'disc_accuracy_change': disc_accuracy_change\n",
    "        }\n",
    "    \n",
    "    def process_dataset(self, df, max_questions=None):\n",
    "        \"\"\"Process complete dataset through pipeline.\"\"\"\n",
    "        self.initialize_all_models()\n",
    "        \n",
    "        # Limit to subset if specified\n",
    "        if max_questions is not None:\n",
    "            df = df.head(max_questions)\n",
    "            print(f\"Processing SUBSET of {len(df)} questions for testing...\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Accuracy tracking\n",
    "        preprocessing_correct = 0\n",
    "        generator_correct = 0\n",
    "        discriminator_correct = 0\n",
    "        total_questions = 0\n",
    "        consensus_questions = 0\n",
    "        adversarial_questions = 0\n",
    "        \n",
    "        # Game theory improvement tracking\n",
    "        gen_initial_correct = 0\n",
    "        disc_initial_correct = 0\n",
    "        gen_improved = 0\n",
    "        disc_improved = 0\n",
    "        gen_degraded = 0\n",
    "        disc_degraded = 0\n",
    "        \n",
    "        print(\"Processing dataset through complete pipeline...\")\n",
    "        print(\"Pipeline: Steps 1-6\")\n",
    "        print(\"  1. Load questions\")\n",
    "        print(\"  2. Phi-2 + Qwen2-1.5B â†’ confidence scores\")\n",
    "        print(\"  3. Filter choices with prob < 0.10\")\n",
    "        print(\"  4. Qwen2-1.5B (Generator) vs Phi-2 (Discriminator)\")\n",
    "        print(\"  5. Nash equilibrium search\")\n",
    "        print(\"  6. Final answers\")\n",
    "        \n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing pipeline\"):\n",
    "            total_questions += 1\n",
    "            \n",
    "            # Process through complete pipeline\n",
    "            result = self.process_single_question(\n",
    "                row[\"question\"], \n",
    "                row[\"choices\"], \n",
    "                row.get(\"answerKey\"),\n",
    "                row[\"subject\"]\n",
    "            )\n",
    "            \n",
    "            # Add original row data\n",
    "            result.update(row.to_dict())\n",
    "            \n",
    "            # Track accuracy\n",
    "            if result['consensus_achieved']:\n",
    "                consensus_questions += 1\n",
    "                accuracy = 1.0 if (result['gen_final_answer'] == result['new_answer_key']) else 0.0\n",
    "                if accuracy == 1.0:\n",
    "                    preprocessing_correct += 1\n",
    "                    generator_correct += 1\n",
    "                    discriminator_correct += 1\n",
    "                print(f\"Consensus: {row.get('id', 'Q' + str(total_questions))} - Correct: {accuracy == 1.0}\")\n",
    "            else:\n",
    "                adversarial_questions += 1\n",
    "                gen_accuracy = 1.0 if (result['gen_final_answer'] == result['new_answer_key']) else 0.0\n",
    "                disc_accuracy = 1.0 if (result['disc_final_answer'] == result['new_answer_key']) else 0.0\n",
    "                \n",
    "                if gen_accuracy == 1.0:\n",
    "                    generator_correct += 1\n",
    "                if disc_accuracy == 1.0:\n",
    "                    discriminator_correct += 1\n",
    "                \n",
    "                # Track game theory improvements\n",
    "                if result['gen_initial_accuracy'] == 1.0:\n",
    "                    gen_initial_correct += 1\n",
    "                if result['disc_initial_accuracy'] == 1.0:\n",
    "                    disc_initial_correct += 1\n",
    "                \n",
    "                # Track accuracy changes\n",
    "                if result['gen_accuracy_change'] > 0:\n",
    "                    gen_improved += 1\n",
    "                elif result['gen_accuracy_change'] < 0:\n",
    "                    gen_degraded += 1\n",
    "                \n",
    "                if result['disc_accuracy_change'] > 0:\n",
    "                    disc_improved += 1\n",
    "                elif result['disc_accuracy_change'] < 0:\n",
    "                    disc_degraded += 1\n",
    "                    \n",
    "                print(f\"Adversarial: {row.get('id', 'Q' + str(total_questions))} - Gen: {gen_accuracy == 1.0} (Î”{result['gen_accuracy_change']:+.1f}), Disc: {disc_accuracy == 1.0} (Î”{result['disc_accuracy_change']:+.1f})\")\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Memory management\n",
    "            if total_questions % 50 == 0:\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "        # Print final summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"COMPLETE PIPELINE RESULTS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"ðŸ“Š Total Questions: {total_questions}\")\n",
    "        print(f\"ðŸ“Š Preprocessing Consensus: {consensus_questions} ({consensus_questions/total_questions*100:.1f}%)\")\n",
    "        print(f\"ðŸ“Š Adversarial Training: {adversarial_questions} ({adversarial_questions/total_questions*100:.1f}%)\")\n",
    "        print(f\"\")\n",
    "        print(f\"ðŸŽ¯ FINAL ACCURACY RESULTS:\")\n",
    "        print(f\"   Preprocessing Only: {preprocessing_correct}/{consensus_questions} = {preprocessing_correct/consensus_questions*100:.1f}%\" if consensus_questions > 0 else \"   Preprocessing Only: N/A\")\n",
    "        print(f\"   Generator (Qwen2): {generator_correct}/{total_questions} = {generator_correct/total_questions*100:.1f}%\")\n",
    "        print(f\"   Discriminator (Phi-2): {discriminator_correct}/{total_questions} = {discriminator_correct/total_questions*100:.1f}%\")\n",
    "        \n",
    "        if adversarial_questions > 0:\n",
    "            print(f\"\")\n",
    "            print(f\"ðŸŽ² GAME THEORY IMPACT (on {adversarial_questions} adversarial questions):\")\n",
    "            print(f\"   Generator Initial Accuracy: {gen_initial_correct}/{adversarial_questions} = {gen_initial_correct/adversarial_questions*100:.1f}%\")\n",
    "            print(f\"   Generator Final Accuracy:   {generator_correct-preprocessing_correct}/{adversarial_questions} = {(generator_correct-preprocessing_correct)/adversarial_questions*100:.1f}%\")\n",
    "            print(f\"   Generator Improvement:      {gen_improved} improved, {gen_degraded} degraded, {adversarial_questions-gen_improved-gen_degraded} unchanged\")\n",
    "            print(f\"\")\n",
    "            print(f\"   Discriminator Initial Accuracy: {disc_initial_correct}/{adversarial_questions} = {disc_initial_correct/adversarial_questions*100:.1f}%\")\n",
    "            print(f\"   Discriminator Final Accuracy:   {discriminator_correct-preprocessing_correct}/{adversarial_questions} = {(discriminator_correct-preprocessing_correct)/adversarial_questions*100:.1f}%\")\n",
    "            print(f\"   Discriminator Improvement:      {disc_improved} improved, {disc_degraded} degraded, {adversarial_questions-disc_improved-disc_degraded} unchanged\")\n",
    "            \n",
    "            # Calculate net improvement\n",
    "            gen_net_improvement = (generator_correct-preprocessing_correct) - gen_initial_correct\n",
    "            disc_net_improvement = (discriminator_correct-preprocessing_correct) - disc_initial_correct\n",
    "            \n",
    "            print(f\"\")\n",
    "            print(f\"ðŸ“ˆ NET GAME THEORY IMPROVEMENT:\")\n",
    "            print(f\"   Generator Net Gain: {gen_net_improvement:+d} questions ({gen_net_improvement/adversarial_questions*100:+.1f}%)\")\n",
    "            print(f\"   Discriminator Net Gain: {disc_net_improvement:+d} questions ({disc_net_improvement/adversarial_questions*100:+.1f}%)\")\n",
    "            \n",
    "            # Quick summary for testing\n",
    "            print(f\"\\nðŸ” QUICK TEST SUMMARY:\")\n",
    "            print(f\"   Game theory helped Generator: {gen_net_improvement > 0}\")\n",
    "            print(f\"   Game theory helped Discriminator: {disc_net_improvement > 0}\")\n",
    "            print(f\"   Overall beneficial: {gen_net_improvement + disc_net_improvement > 0}\")\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def cleanup_all_models(self):\n",
    "        \"\"\"Clean up all models.\"\"\"\n",
    "        self.preprocessing.cleanup_preprocessing_models()\n",
    "        self.models_loaded = False\n",
    "\n",
    "#==============================================================================\n",
    "# STEP 8: MAIN EXECUTION\n",
    "#==============================================================================\n",
    "\n",
    "def test_subset(num_questions=20):\n",
    "    \"\"\"Test the pipeline on a small subset to verify game theory improvements.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"TESTING SUBSET - {num_questions} QUESTIONS\")\n",
    "    print(\"Generator: Phi-2 | Discriminator: Qwen2-1.5B\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Load datasets\n",
    "    arc_df, arc_easy_df = load_arc_datasets()\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = IntegratedPipeline(confidence_threshold=0.10)\n",
    "    \n",
    "    # Test on ARC Challenge subset\n",
    "    print(f\"\\nTesting on {num_questions} ARC Challenge questions...\")\n",
    "    results_test = pipeline.process_dataset(arc_df, max_questions=num_questions)\n",
    "    \n",
    "    # Save test results\n",
    "    test_filename = f\"Data/test_subset_{num_questions}_questions_swapped_roles.csv\"\n",
    "    results_test.to_csv(test_filename, index=False)\n",
    "    print(f\"Test results saved to {test_filename}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    pipeline.cleanup_all_models()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUBSET TEST COMPLETE - SWAPPED ROLES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return results_test\n",
    "\n",
    "\n",
    "def compare_roles_test(num_questions=20):\n",
    "    \"\"\"Compare both role configurations side by side.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(f\"COMPARING ROLES - {num_questions} QUESTIONS EACH\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Test default roles first\n",
    "    print(\"\\n\" + \"=\"*35)\n",
    "    print(\"TESTING DEFAULT ROLES\")\n",
    "    print(\"=\"*35)\n",
    "    results_default = test_subset(num_questions, swap_roles=False)\n",
    "    \n",
    "    # Test swapped roles\n",
    "    print(\"\\n\" + \"=\"*35)\n",
    "    print(\"TESTING SWAPPED ROLES\")\n",
    "    print(\"=\"*35)\n",
    "    results_swapped = test_subset(num_questions, swap_roles=True)\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ROLE COMPARISON SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Extract accuracy info from results\n",
    "    default_gen_acc = results_default.get('generator_accuracy', [0]).mean() if hasattr(results_default, 'generator_accuracy') else 0\n",
    "    default_disc_acc = results_default.get('discriminator_accuracy', [0]).mean() if hasattr(results_default, 'discriminator_accuracy') else 0\n",
    "    swapped_gen_acc = results_swapped.get('generator_accuracy', [0]).mean() if hasattr(results_swapped, 'generator_accuracy') else 0\n",
    "    swapped_disc_acc = results_swapped.get('discriminator_accuracy', [0]).mean() if hasattr(results_swapped, 'discriminator_accuracy') else 0\n",
    "    \n",
    "    print(f\"DEFAULT ROLES (Qwen2=Gen, Phi-2=Disc):\")\n",
    "    print(f\"  Generator Performance: {default_gen_acc:.1%}\")\n",
    "    print(f\"  Discriminator Performance: {default_disc_acc:.1%}\")\n",
    "    print(f\"\")\n",
    "    print(f\"SWAPPED ROLES (Phi-2=Gen, Qwen2=Disc):\")\n",
    "    print(f\"  Generator Performance: {swapped_gen_acc:.1%}\")\n",
    "    print(f\"  Discriminator Performance: {swapped_disc_acc:.1%}\")\n",
    "    print(f\"\")\n",
    "    print(f\"RECOMMENDATION:\")\n",
    "    \n",
    "    default_total = default_gen_acc + default_disc_acc\n",
    "    swapped_total = swapped_gen_acc + swapped_disc_acc\n",
    "    \n",
    "    if swapped_total > default_total:\n",
    "        print(f\"  â†’ Use SWAPPED ROLES (better combined performance)\")\n",
    "    elif default_total > swapped_total:\n",
    "        print(f\"  â†’ Use DEFAULT ROLES (better combined performance)\")\n",
    "    else:\n",
    "        print(f\"  â†’ Both configurations perform similarly\")\n",
    "    \n",
    "    return results_default, results_swapped\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution following the complete pipeline.\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ADVERSARIAL QA SYSTEM - SWAPPED ROLES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Step 1: Load ARC datasets\")\n",
    "    print(\"Step 2: Phi-2 + Qwen2-1.5B â†’ confidence analysis\")\n",
    "    print(\"Step 3: Filter choices with prob < threshold\")\n",
    "    print(\"Step 4: Phi-2 (Generator) vs Qwen2-1.5B (Discriminator)\")\n",
    "    print(\"Step 5: Nash equilibrium search\")\n",
    "    print(\"Step 6: Final answers\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Ask user for test or full run\n",
    "    print(\"\\nChoose execution mode:\")\n",
    "    print(\"1. Test on subset (20 questions) - RECOMMENDED FIRST\")\n",
    "    print(\"2. Test on larger subset (100 questions)\")\n",
    "    print(\"3. Full dataset (1000+ questions)\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1/2/3): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        return test_subset(20)\n",
    "    elif choice == \"2\":\n",
    "        return test_subset(100)\n",
    "    elif choice == \"3\":\n",
    "        # Full dataset processing\n",
    "        arc_df, arc_easy_df = load_arc_datasets()\n",
    "        \n",
    "        # Initialize pipeline\n",
    "        pipeline = IntegratedPipeline(confidence_threshold=0.10)\n",
    "        \n",
    "        # Process ARC Challenge\n",
    "        print(f\"\\nProcessing FULL ARC Challenge dataset...\")\n",
    "        results_challenge = pipeline.process_dataset(arc_df)\n",
    "        \n",
    "        # Save results\n",
    "        challenge_filename = \"Data/complete_pipeline_arc_challenge_swapped.csv\"\n",
    "        results_challenge.to_csv(challenge_filename, index=False)\n",
    "        print(f\"ARC Challenge results saved to {challenge_filename}\")\n",
    "        \n",
    "        # Process ARC Easy\n",
    "        print(f\"\\nProcessing FULL ARC Easy dataset...\")\n",
    "        results_easy = pipeline.process_dataset(arc_easy_df)\n",
    "        \n",
    "        # Save results\n",
    "        easy_filename = \"Data/complete_pipeline_arc_easy_swapped.csv\"\n",
    "        results_easy.to_csv(easy_filename, index=False)\n",
    "        print(f\"ARC Easy results saved to {easy_filename}\")\n",
    "        \n",
    "        # Final cleanup\n",
    "        pipeline.cleanup_all_models()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"COMPLETE PIPELINE EXECUTION FINISHED - SWAPPED ROLES\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(\"All datasets processed successfully!\")\n",
    "        \n",
    "        return results_challenge, results_easy\n",
    "    else:\n",
    "        print(\"Invalid choice. Running test subset...\")\n",
    "        return test_subset(20)\n",
    "\n",
    "\n",
    "def run_full_dataset(swap_roles=False):\n",
    "    \"\"\"Run the full dataset with specified role configuration.\"\"\"\n",
    "    role_desc = \"SWAPPED\" if swap_roles else \"DEFAULT\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FULL DATASET PROCESSING - {role_desc} ROLES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load datasets\n",
    "    arc_df, arc_easy_df = load_arc_datasets()\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = IntegratedPipeline(confidence_threshold=0.10, swap_roles=swap_roles)\n",
    "    \n",
    "    # Process ARC Challenge\n",
    "    print(f\"\\nProcessing FULL ARC Challenge dataset...\")\n",
    "    results_challenge = pipeline.process_dataset(arc_df)\n",
    "    \n",
    "    # Save results\n",
    "    role_suffix = \"swapped\" if swap_roles else \"default\"\n",
    "    challenge_filename = f\"Data/complete_pipeline_arc_challenge_{role_suffix}.csv\"\n",
    "    results_challenge.to_csv(challenge_filename, index=False)\n",
    "    print(f\"ARC Challenge results saved to {challenge_filename}\")\n",
    "    \n",
    "    # Process ARC Easy\n",
    "    print(f\"\\nProcessing FULL ARC Easy dataset...\")\n",
    "    results_easy = pipeline.process_dataset(arc_easy_df)\n",
    "    \n",
    "    # Save results\n",
    "    easy_filename = f\"Data/complete_pipeline_arc_easy_{role_suffix}.csv\"\n",
    "    results_easy.to_csv(easy_filename, index=False)\n",
    "    print(f\"ARC Easy results saved to {easy_filename}\")\n",
    "    \n",
    "    # Final cleanup\n",
    "    pipeline.cleanup_all_models()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPLETE PIPELINE EXECUTION FINISHED - {role_desc} ROLES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"All datasets processed successfully!\")\n",
    "    \n",
    "    return results_challenge, results_easy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68772d37-fff3-4cc4-92be-b51fdfb23ace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
